[{"title":"2020总结","url":"/2021/01/06/2020%E6%80%BB%E7%BB%93/","content":"概述\n2020，自己经历了许多，也成长了许多。\n来到了新阶段，结识了新朋友，学习了新知识，遇到了新挑战…\n当此之时，回望过去的一年，又是感慨良多，在此记录、反思…\n月记事\n信息收集来源：\n1.浏览器历史记录（Dragon Better History插件，其可以方便地按日期查看历史记录）\n2.微信，QQ文件夹\n3.onenote笔记\n4.各种网站搜索记录\n5.网盘备份文件记录\n6.回忆\n\n一月\n\n课程大作业：航天器控制、导航原理\n学习Kalman滤波相关知识\n学习托福\n\n二月\n\n\n本科毕设开题\n\n\n自学计算机组成原理\n\n\n自学小四轴课程 Dragon-fly\n\n\n自学cadence（部分）\n\n\n\n三月\n\n本科毕设开题准备\n自学c语言（小甲鱼课程）\n社区疫情志愿者\n\n\n四月\n\n\n毕设中期答辩\n\n\n垂起方案讨论\n\n\n学习xflow仿真\n\n\n\n五月\n\n毕设中期答辩\n完成毕业论文\n\n六月\n\n毕设定稿及答辩\n课程大作业：\n\n信息与感知：基于超宽带的室内协同定位技术\n仿生机器人：仿生机器人发展综述\n程序设计思想与方法：期末考试 + 寻路算法\n电类学科创新前沿导论：答题大作业\n智能机器人自主定位导航：A*寻路算法实现\n\n\n\n\n七月\n\n垂起倾转双旋翼原型机MARK-I ~ MARK-X\n\n八月\n\n垂起倾转双旋翼原型机MARK-XI ~ MARK-XXI\n\n九月\n\n2020成都先进航电论坛会议\n2020深圳世界无人机大会\n个人博客升级\n\n十月\n\n3D打印飞机一代机 - TinyTrainer 3D设计、制造、试飞\n\n\n\n\n了解异类传感器信息融合相关知识\n\n十一月\n\n学习异类传感器信息融合的相关知识\n3D打印飞机二代机设计\n\n十二月\n\n参加比赛\n论文阅读\n期末大作业\n\n星系与宇宙学大作业：\n\n\n\n飞行力学大作业：\n\n\n\n马克思理论：对当代人工智能异化问题的思考\n\n\n\n最优化方法：凸优化方法与支持向量机分类问题\n\n\n\n2020成果\n\n\n学业\n\n\n本科毕设\n\n\n上海交通大学优秀毕业生\n\n\n\n\n项目\n\n\n纵列式倾转双旋翼无人机\n\n\n全3D打印无人机\n\n\n\n\n论文\n\n成都先进航电论坛中文会议论文一篇\n\n\n\n比赛\n\n**交通·未来大学生科创作品大赛 **- 一等奖\n**科研类全国航空航天模型锦标赛-科技创新作品 **- 三等奖\n第一届“砺剑杯”智能空天创新大赛 - 优胜奖\n“闪铸杯”3D打印创新大赛 - 一等奖\n上海大学生创客大赛 - 三等奖\n\n\n\n","categories":["Life"],"tags":["life","record"]},{"title":"2020新冠居家记","url":"/2020/06/30/2020%E6%96%B0%E5%86%A0%E5%B1%85%E5%AE%B6%E8%AE%B0/","content":"2月份新冠的爆发改变了很多人原来的打算，也彻底改变了我的原计划。这可能是我大学之后最长的一段在家的时光了，所以我觉得有必要记录一下这段独特的经历。\n在家学习\n利用这个大段时间补足我认为本科需要学习但是培养体系没有的课程。遗憾是时间还是太短只补了几门，408没学完，挺可惜。\n多旋翼无人机设计与控制\n简单学习了有关多旋翼无人机设计与控制的知识，课程来自北航可靠飞行控制研究组。课程讲的非常好，可惜的是后续章节我没完全学完。\n\n计算机组成原理\n课程来自计算机组成原理-哈工大精品课程-刘宏伟，跟着b站学习了一遍，同时学着利用onenote记了笔记，然后找了两套试卷简单测试了一下自己学习的效果。\n\nC语言\n感觉自己c之前自学的很不扎实就又系统地学了一遍。课程来自小甲鱼C语言课程，课程讲的很基础，感觉学起来还是挺好的。让我好好打了一下c语言的基础。每讲听完后跟着做课后习题，挺充实。\n\nOneNote &amp; Xmind\nOneNote和Xmind作用不必多说，乘着在家大段的时间利用这两个工具结合了一下自己的学习，熟悉了一下快捷键后发现很好用。（可惜当时接触markdown没去细看，不然用markdown更爽）\nOneNote快捷键\nOneNote Gem\nXmind\nXmind 我现在用的不多了，因为基于OneNote的标题体系记录的笔记我感觉已经可以代替思维导图了，例如下图\n\n毕业设计\n毕业设计是有关多无人机协同目标跟踪的，比较偏算法。我用matlab编写了一个仿真程序，然后比较了各种融合算法在仿真中的效果。受到疫情影响无法进行实际平台实验，所以后来又增加了有关基于student-t分布的融合算法的内容。\n分布式多无人机系统目标跟踪技术\n仿真程序\n结构其实很简单，主要是先前对模块化和接口这些我了解比较少，所以改了两三版使得它更加模块化了。我列了一个简单的程序结构图\n\n基于高斯分布的融合\n这个也是非常基础的内容了，对比了几种滤波方法+分布式信息融合方法在该仿真程序上的效果\n\nCKF + CC（简单凸融合）\nCKF + CI （协方差交叉）\nCKF +  BSC (Bar Shalom-Campo)\nUKF + CI\n\n基于student-t分布的融合\n哈工程的黄玉龙博士（后来被破格录用副教授了）在16年提出了一种鲁棒性容积学生t滤波器（其实就是把CKF的分布换成了学生t分布），这种滤波器在传感器具有野值下相比于传统的卡尔曼滤波器具有更好的滤波效果（主要原因是学生t分布能比高斯分布更好地刻画具有野值噪声的重尾特性）\n所以我毕设的后期就是将重尾噪声考虑了进来，利用该滤波器和简单凸融合算法又得到一种分布式信息融合算法CST-CC，然后把它和CKF-CC进行了对比。效果在追踪直线运动目标时其实显得也不是很突出。\n\n毕设的总结\n感觉这个毕设还是比较简单的，实际上我也没花太多时间，代码量也不是很大。主要的意义在于了解了一些有关滤波的背景知识，也有了一个比较好的问题切入点。后续把毕设的主要内容投稿到了成都航电所搞的高端航电论坛上被接收了，虽然是中文水会，第一次被接收还是有点小触动的。\n课程项目\n大四下我觉得自己电类方面的知识实在是过于浅薄，无奈已经无法再选到十分solid的课程了，所以选了5门与电类相关的选修课程，平时听听网课还是很不错的。\n\n信息与感知：主要讲传感器的，没怎么细听，感觉比较水\n程序设计与方法：cs的陆朝俊老师讲的，主要用python，有平时作业和四次上机，包括最后还有上机考试和单人项目大作业，可以说是相当的充实了。\n仿生机器人：微纳中心张卫平老师的课程。由于我之前也做过一点扑翼机就选了。平时作业主要是做做调研，挺轻松的。老师对我也很亲切，一直小王小王地叫我hhh\n无人机总体设计：航模队吴俊琦老师的课程，吴老师的课还好，负担也不大，主要我们还在搞cadc所以课程刚好可以和这个叠起来，没增加太多负担\n机器人自主定位导航：这个课程也是挺有意思，老师讲了前几周之后就是企业的老师来讲了。企业老师主要是做SLAM的，有意思的是最后可以在家远程在课堂上的机器人上跑代码，让它做动作；最后大作业是实现A*算法，我也由此接触了更深一点的面向对象的编程。\n\n玩耍\n四月份之后我的心仿佛也随着春天温度的升高和疫情形式的逐渐好转好起来了一点，所以和朋友们愉快地玩耍起了王者荣耀、炉石传说。王者打到星耀王者去了，炉石的话好像只是钻石，不过后来酒馆战旗玩起了整活鱼人和养蜥流真是十分的有意思。\n\n养蜥流\n\n整活鱼人\n社区疫情志愿者\n在疫情期间我也参加了社区的志愿者服务活动，其实主要工作就是用我的mavic mini去拍楼顶或者小区内聚集晒太阳的人，然后通知他们赶快疏散（回家前带无人机我也没想到还有这作用。。。）航拍着空荡荡的街道，感觉还是很有趣的，而且下午可以乘着这个去高地走走呼吸下新鲜空气还是很惬意的。\n","categories":["Life"],"tags":["life","COVID-19"]},{"title":"Sensor and Sensor Fusion Technology in Autonomous Vehicles:  A Review | Sensors","url":"/2021/04/18/2021%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%B8%AD%E7%9A%84%E4%BC%A0%E6%84%9F%E5%99%A8%E5%92%8C%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E7%BB%BC%E8%BF%B0/","content":"sensor fusion\n这篇主要还是总结了一下关于异类融合的方法、算法。\n相机、激光雷达、雷达传感器的特点对比\n这张表清晰地给出了他们的优劣势，有力地回答了一个问题：我们为什么要研究异类信息融合？\n说白了就是：取长补短，得到总体更好的感知效果。\n\n三种主要的传感器融合\n\nCL (camera-LiDAR)\nCR (camera-radar)：CR是目前应用最广泛的，CLR次之，然后是CL.[21]；Tesla还利用CR和其他传感器例如超声波传感器融合来得到车辆的环境。[8]\nCLR (camera-LiDAR-radar)：通过激光雷达得到精度更高，范围更大的结果。Waymo and Navya[179]等人就利用了这种融合\n\n传感器融合的方法对比\n目前还是按照融合框架分为三种：\nHLF (high-level fusion)\n​\t在HLF方法中，每个传感器独立进行目标检测或跟踪算法，然后进行融合。\n\n例子：参考文献[30]利用HLF方法将处理后的数据，即雷达信号与激光雷达点云独立融合，然后使用非线性卡尔曼滤波方法检测障碍物和状态跟踪。\n\n​\t通常采用HLF方法是因为相对的复杂度比LLF和MLF方法要低。然而，HLF提供的信息不充分，例如，如果存在若干重叠障碍，则丢弃具有较低置信值的分类。\nLLF (low-level fusion)\n​\t相反，在LLF方法中，来自每个传感器的数据在最低抽象级别(原始数据)进行融合。因此，所有的信息都被保留下来，可以潜在地提高障碍物检测的准确性。\n\n例子：文献[181]提出了一种两阶段的3D障碍物检测体系结构，命名为3D-cross view fusion(3D- CVF)。在第二阶段，他们利用LLF方法，利用基于3D感兴趣区域(RoI)的池化方法(3D region of interest(ROI)-based pooling)，将第一阶段获得的相机-激光雷达联合特征图(camera-LiDAR joint feature map)与低级相机和激光雷达特征(low-level camera and LiDAR feature)融合。他们在KITTI和nuScenes数据集上评估了所提出的方法，并报告称，物体检测结果优于KITTI排行榜上最先进的3D物体检测器(更全面的总结见参考文献[181])。\n\n​\t在实践中，LLF方法带来了许多挑战，尤其是在其实现方面。它需要精确的外部校准传感器，以准确地融合其对环境的感知。传感器还必须补偿自我运动，并进行时间校准[180]。\nMLF (middle-level fusion)\nMLF，也称为特征级融合，是LLF和HLF之间的一个抽象级别。它将从相应的传感器数据(原始测量值)中提取的多目标特征进行融合，如图像中的颜色信息或雷达和激光雷达的位置特征，然后对融合后的多传感器特征进行识别和分类。\n\n例子：文献[182]提出了一种特征级传感器融合框架，用于在通信能力有限的动态背景环境中检测目标。他们利用符号动态滤波(Symbolic Dynamic Filtering, SDF)算法，在环境光强变化的情况下，从多个红外传感器中提取不同方向的低维特征，然后将提取的特征与运动目标的凝聚层次聚类算法融合成簇检测。\n\n​\t然而，由于MLF对环境的感知有限并丢失了上下文信息，似乎不足以实现SAE 4级或5级的自动驾驶系统[183]。\n下表清晰地展示了各种融合方法的描述和优劣势：\n\n融合算法（工具）\n​\t上一部分讲解的是一些抽象的融合架构，这一部分更加关注于具体的算法，或者说：工具。这些经典的目标检测和分类算法可以作为我们的网络的主干结构。下面介绍几种常用的。\nYOLO\n用于二维图片目标检测\nYou only look once(YOLO)在2016年被提出，成为了目标检测算法的一个重要的里程碑事件[188]。\n\n例子：文献[187]提出了一种先进的加权平均YOLO算法来融合RGB相机和激光雷达点云数据，以提高目标检测的实时性能。\n\n\n文献[190]中，作者提出了一种基于cnn的方法，利用近红外光和热感摄像机通过情绪来检测攻击性驾驶行为。他们利用CNN输出的近红外光图像和热图像的分数进行分数级融合，以提高检测精度。他们提出的方法获得了较高的情绪分类精度，并证明了他们提出的技术比传统的情绪检测方法取得了更好的性能。\n\nVoxelNet\n用于三维目标检测，处理3D点云数据\n\n文献[191]VoxelFusion: 利用了他们之前在[192]中提出的VoxelNet框架，提出了两种特征级融合方法，即PointFusion和VoxelFusion，将RGB和点云数据结合起来进行3D对象检测。VoxelFusion方法将VoxelNet创建的非空3D体素投影到图像上，并在2D的ROI区域中提取特征，从而在像素水平上能和之前的图像特征进行叠加(concatenate)融合\n\n\n文献[193]PointFusion: 提出了一个点融合框架，利用图像数据和原始点云数据进行3D对象检测。他们利用CNN和PointNet[194]架构分别对图像和点云进行处理，然后将结果输出组合起来以预测多个3D检测框的假设位置及其置信度。PointNet体系结构是一种新颖的神经网络，为处理原始点云数据的3D分类和场景语义解析等应用提供了统一的体系结构。\n\n其他基于深度学习的传感器融合算法包括:\n\n[195]：ResNet，或残差网络，是一种残差学习框架，有助于深度网络的训练。\n[196]：SSD (Single-Shot Multibox Detector)是一种将包围盒离散为一组不同尺寸和长宽比的盒子，以检测不同尺寸的物体的方法[196]，它克服了YOLO小尺度和变化尺度的物体检测精度的限制\n[197]：CenterNet代表了最先进的单目摄像机三维物体检测算法，该算法利用关键点估计来找到包围盒的中心点，并将中心点回归到所有其他物体属性，包括尺寸、3D位置、方向和姿态。\n\n\n\n参考阅读资料\n\n\nVoxelNet论文阅读\n\n\n【SSD算法】史上最全代码解析-核心篇\n\n\n深度学习 – SSD 算法流程详解\n\n\n扔掉anchor！真正的CenterNet——Objects as Points论文解读\n\n\n","categories":["Review"],"tags":["sensor fusion"]},{"title":"Patches Are All You Need?","url":"/2021/11/16/Convmixer/","content":"论文地址\n代码\n摘要\n\n\n背景：ViT的性能提升是因为Patches还是因为自注意力？\n\n\n工作：\n\n\n提出了ConvMixer，一个和ViT、MLP-Mixer相似的网络。\n\n\n相似在什么地方呢？：ConvMixer直接以patch分割作为输入，分离了空间维度（spatial）和通道维度（channel）的混合，并在整个网络中保持相同的大小和分辨率\n\n\n不同之处呢？：ConvMixer只使用标准卷积来进行Mixing步骤\n\n\n\n\n结果：\n\nConvMixer：简单（仅仅几行代码！），优于ViT、MLP-Mixer和ResNet等一系列结构\n\n\n\n介绍\nTransformer（以下简称TF）被应用到视觉任务上，但是由于自注意力机制的平方级别的计算量，无法对图片直接进行像素维的计算，所以，一个妥协（compromise）的方式就是：把图片分割为一个个patch，然后对patch进行线性嵌入为向量后，直接送入TF进行处理。\n但是在本次工作中，我们发现，ViT强大的性能可能更多地来自这个基于patch的表示，而不是TF结构本身！我们由此提出了一个简单的网络，名为“ConvMixer”，因为它和MLP-Mixer十分相像：\n\n\n直接对patch进行操作\n\n\n在所有层都保持了相同的分辨率和尺寸表示\n\n\n它不在连续的层之间进行下采样\n\n\n它分离了信息的“通道维混合”和“空间维混合”\n\n\n但是不同指出在于：\n\nConvMixer只使用标准卷积\n\n当然，我们的这些结果只是类似于一个“快照”（snapshot），但是我们相信它提供了一个“基于patch但是卷积”的网络的基线，这样的更加强大的网络可能在未来出现。\n一个简单的模型：ConvMixer\n模型结构：\n\n模型结构很简单（几乎都不需要解释把= =），前面主要是利用一个kernel size = stride的卷积核进行了patch分割，激活然后BN。后面ConvMixer Layer采用了可分离卷积，其中depthwise采用了残差连接结构。最后Global Average Pooling跟一个FC形成head，其实作者也认为这个head如果改改设计应该还能提升。\n设计动机：\n​\t设计思路延续了MLP-Mixer的思想，其中depthwise和pointwise分别来进行空间（spatial）和通道（channel）维度的混合（mixing）。MLP的一个关键的设计思想是利用self-attention来混合远距离空间上的位置（distant spatial location），也就是说它可以拥有任意大小的感受野。所以，我们采用非同寻常的大卷积核来达到类似的效果。\n​\t诚然，MLPs和self-attention理论上具有更大的灵活性，允许超大的感受野和内容认知行为（content-aware behavior），但是卷积的归纳偏置使得它天生适合视觉任务并因此具有很高的数据有效性。通过使用这种标准操作，我们还可以一窥与传统的金字塔形、渐进式下采样的卷积网络设计相比，patch表示的根本作用是什么。\n实验\n\nCIFAR-10:仅用0.7M参数达到了96%准确率\n训练设置：\n\n数据集：ImageNet-1k，无额外数据\n使用了一些timm框架下的数据处理方法\n优化器AdamW\n由于计算资源限制，没有对超参数进行调参\n\n\n结果\n\n\n源码阅读\n由于代码很简单直接看一下\nimport torch.nn as nnclass Residual(nn.Module):    def __init__(self, fn):        super().__init__()        self.fn = fn    def forward(self, x):        return self.fn(x) + xdef ConvMixer(dim, depth, kernel_size=9, patch_size=7, n_classes=1000):    return nn.Sequential(        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),        nn.GELU(),        nn.BatchNorm2d(dim),        *[nn.Sequential(                Residual(nn.Sequential(                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=&quot;same&quot;),                    nn.GELU(),                    nn.BatchNorm2d(dim)                )),                nn.Conv2d(dim, dim, kernel_size=1),                nn.GELU(),                nn.BatchNorm2d(dim)        ) for i in range(depth)],        nn.AdaptiveAvgPool2d((1,1)),        nn.Flatten(),        nn.Linear(dim, n_classes)    )\n其中\nnn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)# patch分割\nnn.Conv2d(dim, dim, kernel_size, groups=dim, padding=&quot;same&quot;),#深度维卷积，输入/输出维数不变，信息只在spatial维度上交流\nnn.Conv2d(dim, dim, kernel_size=1)#通道维卷积，1*1卷积核，信息在channel维度上交流\n读后感\n\nspatial和channel维度的信息交流分开交替进行。（猜测：这样做主要是降低了网络的学习负担？，梯度的来源比较“纯净”？）\n目前看来，关键不是自注意力还是卷积，而是网络是否保证了全部layers都有很大的感受野（patch分割之后，大卷积核也能产生很大的感受野）\n\n","categories":["Paper Reading"],"tags":["transformer - deep learning"]},{"title":"RRPN: Radar region Proposal network for object detection in autonomous vehicles|IEEE ICIP 2019","url":"/2021/04/19/RADAR%20REGION%20PROPOSAL%20NETWORK/","content":"Author\n\nRamin Nabati\nQi Hairong\nFrom: Department of Electrical Engineering and Computer Science The University of Tennessee Knoxville, USA\n\nAbstract\n\n背景：目前在最先进的两阶段法的目标检测网络中，候选区域搜索算法成为了一个影响速度的主要原因\n我们的工作：\n\n提出了RRPN，利用雷达信息实时提供候选区域搜索框的算法\n使用Fast-RCNN在nuSence上测试，我们的算法速度比对比对象快了100多倍\n并且同时具有更高的精度和召回率\n\n\n\nIntroduction\nradar的应用\n\n[4]：最早提出想用radar的一批人提出了一些应用方式，但是很无奈，早期缺乏雷达数据集缺乏没法做\n\n2D目标检测网络\n\n\n[2,5,6,7]：借助于CNN，2D的目标检测近些年来进展巨大。这些方法主要分为了两类：一阶段法和二阶段法\n\n\n[8]：一阶段法将目标检测看作一个回归任务，直接从图像学习目标类的概率和检测框\n\n[9]：YOLO\n[7]: SSD\n\n\n\n[2,6]: 两阶段法，在第一阶段使用RPN（region proposal networks）生成ROI区域，然后在第二阶段应用这些区域进行分类回归\n一阶段法的准确率低但是速度往往比较快，二阶段法的”瓶颈“是RPN，它需要对单张图片生成ROI区域来进行后续检测\n\n\nRelated Work\n雷达相关工作\n\n\n[10]: 雷达导航，EKF\n\n\n[11]：一种基于相关的模式匹配算法，加上距离窗口来检测和跟踪车辆前方的目标。\n\n\n[12]：Ji等人提出了一种基于雷达检测的注意力选择系统。利用雷达检测来选择候选目标并且生成一个窗口分别进行识别\n\n\n融合相关工作\n\n\n[13]：提出了一个基于激光雷达和视觉的行人检测系统，使用了集中和分散的融合架构。在前者中，作者提出了一种特征级融合系统，将来自激光雷达和视觉空间的特征合并到一个向量中，并使用单一分类器进行分类。后者采用两个分类器，每个传感器和特征空间一个分类器。\n\n\n[14]：Choi等人提出了一种多传感器融合系统，解决了汽车上集成的14个传感器的融合问题。该系统使用扩展卡尔曼滤波器处理来自单个传感器的观测结果，能够检测和跟踪行人、自行车和车辆。\n\n\n基于视觉的目标推荐算法\n在目标检测网络中，基于视觉的目标提议算法是一种非常流行的算法。\n\n[3]：选择性搜索算法，通过使用多种互补的图像分区来实现对目标的多样化搜索。尽管有很高的准确性，选择性搜索在计算上是昂贵的，每幅图像运行2-7秒。\n[15]：边缘盒是另一种基于视觉的对象建议算法，使用边缘来检测对象。Edge Boxes比选择性搜索算法要快，每幅图像的运行时间为0.25秒，但在自动驾驶等实时应用中，它仍然被认为是非常慢的。\n\nRADAR REGION PROPOSAL NETWORK\n​\t利用雷达来进行region proposal，不仅能够加快算法的速度，还能利用雷达具有的距离和位置信息来标记识别的目标。\n​\tRRPN实际上还提供了一种注意力机制，它使得计算资源集中在重要的区域上。\n​\tRRPN包括三个步骤：视角转换、锚点生成和距离补偿，每一个都将在下面的章节中单独讨论。\n视角转换\n第一步就是要把雷达点在图像上标记出来，我们使用投影的方式。雷达一般是鸟瞰图（图1（a））的形式\n3D point : P = [X; Y; Z; 1]\nits image：p = [x; y; 1]\n则他们之间的转换关系一般为下式\n\n其中H是根据相机位置得到的参数\n\n锚点生成\n投影之后的雷达点叫做POI（point of interest），它提供了目标在像平面内很有价值的信息，接下来我们要在POI附近生成ROI。但是还有两个问题：\n\nPOI往往不是刚好在目标的中心的、\n雷达并不提供检测到的目标的尺寸信息，如果我们采用固定大小的检测框那样就不是很有效了。\n\n我们利用了Fast-RCNN生成锚点检测框的思想来缓解以上问题，做法是：\n\n以POI为中心生成不同大小和不同长宽比的检测框，如图1(b)，我们总共用了4种不同大小和3种不同长宽比的检测框\n由于POI不是经常在中心，我们还采用了不同的偏移版本，如图1（c-e）\n\n距离补偿\n目标在像平面中的大小与他们的距离直接相关，这也决定了锚点检测框的大小应该是多少，我们利用雷达刚好可以方便地获得距离，所以我们利用下列公式来修正锚点检测框\n\n是第i个目标的距离，，是两个调节参数，这两个参数通过对比和真值检测框的IOU达到最大来进行学习。如下列公式所示\n\n\n是训练图片的个数,是第i张图片中的真值检测框，是生成的锚点，是第i张图片中第j个真值检测框和第k个锚点。这个方程是用来找到使得真值和推荐检测框的IOU最大的，。我们使用了简单格子搜索法来找到，\n实验及结果\n数据集\n\n数据集：nuScenes\n测试类：汽车、卡车、人、摩托车、自行车和公共汽车\n两部分：NS-F来自前视，目标比较远；NS-FB来自后视，难度更大\n训练-测试：0.85-0.15\n\n实施细节\n\n\nRRPN+Fast-RCNN\n\n\n主干:两个不同的 ResNet-101, 记为R101 and ResNeXt-101, 记为X101\n\n\n训练阶段：先从COCO数据集上预先训练\n\n\n和SS（Selective Search algorithm）选择搜索算法进行了比较\n\n\n结果\n\n\nAP,AR都有所上升。\n参考阅读\n\nregion proposal algorithm 透彻理解RPN: 从候选区域搜索到候选区域提取网络\n\n\n","categories":["Paper Reading"],"tags":["sensor fusion","radar","region proposal"]},{"title":"Squeeze-and-Excitation Networks（SENet）","url":"/2021/11/16/SENet/","content":"论文地址\n代码地址\n介绍\n本篇论文提出了SENet,获得了最后一届ImageNet比赛的冠军.值得注意的是,它并没有采用特别复杂的结构,提出了一种通道自注意力机制,筛选更有意义的通道特征.本篇只是一个总结,参考了知乎,csdn上的一些论文解读.\n解读Squeeze-and-Excitation Networks（SENet）\n网络\n\n\nFtr:常规特征提取网络\nFsq: Global Average Pooling(GAP),对每一层特征进行均值池化.\nFex: 两层全连接,其中还有一个激活层,输入和输出经过Fex维度不改变,但是线性层的中间层维度比C小,为C/r.\nFsacle: 进行sigmoid后映射到[0,1]上,然后对原特征图进行加权\n\n几个问题\n\n为什么GAP是很简单地进行均值池化?\n\n答:因为最终的scale对通道特征起作用,使用全局均值池化可以消除&quot;空间&quot;的相关性影响,而只保留&quot;通道&quot;上的相关性来形成注意力.\n\n为什么需要两层的线性层?\n\n知乎上讲的特别好!这里我直接借用过来.以下是三种方式的对比:\n\n图2最上方的结构: 没有匹配整个数据集直接GAP后进行加权,存在的问题是没有匹配整个数据集,而是只根据一个batch的信息进行相关性计算.\n图2中间是经典的卷积结构: 空间,通道混乱有人会说卷积训练出的权值就含有了scale的成分在里面，也利用了通道间的相关性，为啥还要多个SE Block？那是因为这种卷积有空间的成分在里面，为了排除空间上的干扰就得先用GAP压缩成一个点后再作卷积，压缩后因为没有了Height、Width的成分，这种卷积就是全连接了。\n图2最下面的结构: 张冠李戴SE模块和传统的卷积间采用并联而不是串联的方式，这时SE利用的是Ftr输入X的相关性来计算scale，X和U的相关性是不同的，把根据X的相关性计算出的scale应用到U上明显不合适。\n说白了,SENet其实提出的是一种方便的计算通道注意力的方式!类似于一个模块可以增加到自己的网络\n\n论文中给出的两个例子\n\n\n分别SE模块是和Inception和residual module结合的例子\n\nSE模块的位置\n\n\n作者还在消融实验中测试了这四种不同位置的SE模块的效果,发现只有POST版的性能有所下降,这表明SE模块具有很强的鲁棒性,而且最好是被放在靠前的位置.\n\n","categories":["Paper Reading"],"tags":["Channel Attention - deep learning - CVPR 2018"]},{"title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows","url":"/2021/11/18/SwinTransformer/","content":"论文信息\n\n2021 ICCV bestpaper\n微软亚洲研究院\n论文地址\n项目地址\n\n摘要\n\n\nVision Transformer的两个问题：\n\n\n两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。\n\n\nCV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大\n\n\n\n\n作者的解决方案：\n\n\n引入CNN中常用的层次化构建方式构建层次化Transformer\n\n\n引入locality思想，对无重合的window区域内进行self-attention计算\n\n\n\n\n介绍\nWindows\n首先看看Windows到底是怎么做的？\n\n右侧：ViT的做法：\n\n先来看看经典的做法，ViT把图片分成一个一个16*16的像素小块，我们常叫做patch。然后对每个patch拉成“向量”后，经过一个线性层变成Transformer网络需要的维度（还需要加入位置编码）。之后就是传统的Transformer计算了\n这样就使得，每个patch代表一部分图片的信息，并且每层小encoder都有全局的感受野。但是这样带来的问题是图片尺寸大了的话，还用小的patch块就会导致patch的数量很多，计算量平方倍的增长。\n\n左侧：Swin T的做法：\n\n简单来说，重要的就两点：\n\n每一层的patches分割是不一样的，浅层分割的patch更小，随着网络加深，慢慢合并（类似cnn的思想）\n每一层都只计算局部自注意力（在红色框框即Windows内进行自注意力计算），这样计算量随输入图片尺寸线性增长，可以接收了，所以可以做一些场景分割等对高分辨率有较高要求的工作\n\n\n\nShifted Windows\n等等，以上好像只说了Windows，那Shifted又是什么呢？\n\n可以看出，Shift的意思就是在下一层，窗口的分区会进行偏移，有些窗口会被生成在前一个窗口的边界上，以连接这些窗口。注意这和Sliding Windows的\n作者提出的Swin-Transformer在分类、检测、语义分割任务上都获得了SOTA。作者希望基于Transformer，并且能在多个图像相关的任务都达到SOTA的swin-transformer模型，能对未来人们发现更统一的模型做出贡献，真正打通我们对图像信息和文本信息的建模的认知。\n相关工作\n\nCNN：\n基于自注意力SA的框架\n给CNN增加自注意力、Transformer\n基于Transformer的视觉backbone\n\nViT：无重叠、中等尺度的patch、实现了速度-精度的完美平衡；需要大数据集预训练\nDeiT：利用几种训练方法使得ViT能在ImageNet-1K数据集上训练，但是还是不适合需要高分辨率的图像任务\n还有一些直接利用ViT，对其进行下采样或者反卷积后进行目标检测的方法，但是都没有获得特别好的结果\n在基于ViT结构的分类方法中，Swin-Transformer也是实现了最佳的“速度-精度平衡”，即使它是用来作为一个一般化的backbone，没有对该任务进行优化\n也有一些人尝试利用Transformer获得高分辨率的特征图，但是依然受限于随图片尺寸二次增长的计算复杂度，而我们的方法计算复杂度线性增长\n我们的方法是有效并且高效的，同时在COCO目标检测和ADE20K语义分割两个任务达到了SOTA。\n\n\n\n方法\n总体结构\n概览\n\n\n\n首先，将图片分割为patches，patches的大小这里设置的是4×4，所以每个patch（token）的维度就是4×4×3=48，每个patch的特征的值就是原始的RGB数值。\n\n\n阶段1：对进行一个线性层，可以把每个token的维度降低到C，然后在阶段1后面的Swin Transformer Block中，使用Modified Self Attentio进行计算，保持tokens的数量不变。\n\n\n阶段2：为了进行分层表示，如之前所介绍的，开始合并patches。在这一层以2×2为一组对patches进行合并。所以每个patch的维度变为4C，总共的patches的数量缩小4倍（相当于原来patch的大小是4×4，现在是8×8）。然后再进行一次线性变换，使得输出的维度变为2C.\n\n\n阶段3、阶段4：类似于阶段2，\n\n\nSwin Transformer Block\n将原来的Transformer Block的MSA替换为shifted windows multi-heads self attention，SW-MSA。其他结构保持不变\n基于偏移窗口的自注意力机制\n基于无重叠窗口的自注意力机制\n不同于传统的全局的自注意力，我们的自注意力在不重叠窗口内部进行，假设总共有h×w个patches，每个窗口包括M×M个patches，那么两种机制的计算复杂度为\n\n可见，MSA和patch数量hw成平方关系，而W-MSA在M固定的情况下，和hw只是线性关系。\n在连续的block间进行偏移窗口分区\n仅仅基于窗口的自注意力没有窗口之间的信息交流，限制了其能力。所以增加偏移窗口。那么怎么偏移呢？\n其实它的做法也不复杂，通速地说，从当前的windows的中心点作为分割点，这里我画了一个草图：\n\n对于偏移参数的高效batch设置\n偏移窗口的一个问题就是增加了窗口的数量，并且会产生一些小于M×M尺寸的窗口，一种比较原始的方法就是把这些小的窗口填充（pad）到M×M。\n这样做带来的计算量提升是很大的，比方说上面的例子中，相当于我们从2×2个窗口变成了3×3，计算量增大了2.25倍\n为了解决这个问题，作者提出了cycle-shift方法，如下图所示：\n\n这里还是比较巧妙的，具体步骤看附录，借用了陀飞轮画的图和解释，感谢！\n相对位置偏差\n在计算注意力时，我们参考其他人的工作，增加了一个相对位置偏差项B：\n\n我们也尝试了绝对位置编码，效果较差一点点，我们没有采用\n模型参数设置\n\n\n四种模型\n\n\nB：Base，计算复杂度和ViT/DeiT相当\n\n\nT/S/L: Tiny/Small/Large，基础模型0.25，0.5，2倍的模型大小和计算复杂度\n\n\nT、S模型的复杂度分别和ResNet-50 (DeiT-S) and ResNet-101相当\n\n\n\n\nwindows size：M=7\n\n\nquery dimension of each head：d=32\n\n\n每层MLP的膨胀度：α=4\n\n\n\n实验\n分类\n\n目标检测\n\n语义分割\n\nshift window和位置编码的影响\n\n总结\n主要的两个思想：\n\nWindows尺度变化：随着网络变深，将原来细分的patches不断合并，借鉴了CNN的尺度变化。把前面层的patches分割的十分小，从而支持高分辨率的任务；而后面不断合并，汇总patch的信息。\n偏移窗口自注意力：采用窗口机制来进行局部自注意力，并且利用shifted机制来进行窗口之间的信息交流。成功把自注意力的计算量需求降低到线性增加。\n\n附录\ncycle shift\n\n","categories":["Paper Reading"],"tags":["transformer - deep learning"]},{"title":"hexo使用中的一些小问题：总结&避坑","url":"/2021/03/15/hexo%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E9%97%AE%E9%A2%98/","content":"title中有其他字符怎么办（例如冒号）\n例如我的标题应该是：\ntitle: Sensor and Sensor Fusion Technology in Autonomous Vehicles:  A Review | Sensors\n然后使用hexo g之后即报错：\nERROR {  err: YAMLException: incomplete explicit mapping pair; a key node is missed; or followed by a non-tabulated empty line at line 1, column 66:       ... echnology in Autonomous Vehicles: A Review | Sensors\n那么我的第一反应是检查：\n\ntitle后面的冒号应该是英文冒号，最好后面要跟一个空格\n\n然而发现不是，后来发现是因为标题中有特殊字符(:)导致报错，百度了一下没有很好的解决方法，google后看到大神给出了一个最简单粗暴的方法:\n\n只需要在标题外面加上“ ”即可！\n\ntitle: \"Sensor and Sensor Fusion Technology in Autonomous Vehicles:  A Review | Sensors\"\n在Windows Shell上hexo d不成功\n笔者之前习惯在window shell中上传，后来发现很多次明明自己可以很好地登录github却hexo d之后却长时间没反应，这个小问题也挺烦人的。\n后来发现使用git bash上传之后，几乎没有遇到这个问题！\nwindows下快速打开git bash的方法\n\n\n选择你hexo所安装的文件夹，我这里是C:\\Users\\搞事boy\\Documents\\blog\n\n\n\n2.右键，点击git bash here即可打开bash命令行\n\n\n后续操作都是一样的（hexo clean、hexo g、hexo d）\n\nMathjax公式\nMathjax给我提供了很好看的latex格式公式，但是也存在一些小毛病\n矩阵换行问题\n正常的latex用两个反斜杠表示换行，代码如下：\n$$p=H P, \\quad H=\\left[\\begin{array}{llll}h_{11} &amp; h_{12} &amp; h_{13} &amp; h_{14} \\\\h_{21} &amp; h_{22} &amp; h_{23} &amp; h_{24} \\\\h_{31} &amp; h_{32} &amp; h_{33} &amp; h_{34}\\end{array}\\right]$$\n效果是：\n\n并没有换行\n改用四个反斜杠的话\n$$p=H P, \\quad H=\\left[\\begin{array}{llll}h_{11} &amp; h_{12} &amp; h_{13} &amp; h_{14} \\\\\\\\h_{21} &amp; h_{22} &amp; h_{23} &amp; h_{24} \\\\\\\\h_{31} &amp; h_{32} &amp; h_{33} &amp; h_{34}\\end{array}\\right]$$\n可以换行\n\n不知道这是一个bug还是什么原因？可能是第一个反斜杠表示转义了？\n","categories":["Life"],"tags":["hexo skill","hexo"]},{"title":"MathJax Test","url":"/2020/09/12/test_mathpix/","content":"\n这是一个行间公式测试\n","categories":["MathJax"],"tags":["MathJax"]},{"title":"十分钟飞翼设计要点","url":"/2020/10/30/%E5%8D%81%E5%88%86%E9%92%9F%E9%A3%9E%E7%BF%BC%E8%AE%BE%E8%AE%A1/","content":"前言\n十分钟学会设计飞翼飞机的要点，主要知识翻译学习于外网。\n目标：让有基础的力，力矩，平衡概念的读者也能快速明白和理解如何设计飞翼飞行器\n翼形\n其实，只要我们选择合适的后掠(sweep)和扭转(twist)设计，那么大部分的翼形都可以作为飞翼和无尾翼布局飞机的翼形。因此，所谓的特有的“飞翼翼形”其实是不存在的。\n然而，如果我们想要设计一个具有较宽可操纵范围的无尾翼布局的飞机，那么机翼就应该需要有较小的扭转角（这是为了在整个飞行包线中始终将诱导阻力保持在一个合理的范围内）。在这个条件下，机翼的力矩系数随攻角的变化程度一定不能过大，所以，我们最好采用低力矩系数的翼形。如果是无后掠的情况，那么我们甚至必须要采用具有正力矩系数的翼形，以避免在配平的飞行中需要向上配平襟翼，这种具有正力矩系数的翼形，通常都具有反折型（s型）的中弧线(reflexed camber line)\n纵向稳定性\n就如同航模飞机他们的天上飞的那些全尺寸的大“堂兄弟”们一样，航模飞机也需要一定的稳定性：即，在飞行中受到小扰动后自动恢复原始状态的能力。至于到底需要多少稳定性，这完全取绝于飞行员的“个人口味”：老鸟喜欢低稳定裕度的飞机，而新手大多喜欢高稳定性裕度的飞机。接下来将介绍如何对于飞翼，我们如何预测重心位置，以及找到合适的后掠和弧度曲线。\n无后掠翼设计\n对于常规布局飞机，水平尾翼提供了飞机所必需的纵向稳定性。但对于稳定的无后掠翼飞翼飞机，稳定飞机的只能是主翼。在大多数情况下，常常采用反折(s形)弧度线的翼形实现纵向稳定性。\n一些重要的气动和机械知识\n为了理解为什么反折型翼形提供了纵向稳定性，这两个概念非常重要：\n\n\n合力和合力矩，C/4点：almostly，AC\n作用在机翼各个面上的压力，可以通过一个作用在一个点上的合力和合力矩代替。通常，都将这个点选为四分之一弦长点。当攻角改变时合力将会改变，而合力矩近乎不变（这和c/4点靠经气动中心有关）。攻角增大，合力就会增大。（失速前）\n\n\n重心：GC点\n重心就像飞机的“质心”，飞机的各种转动，都是绕着这个点在动。而飞机的平行移动则是这个点的平移。\n\n\n平衡性\n为什么无尾翼无后掠时要采用（具有正力矩系数的）s型翼形？请看下面的分析。\n\n常规弧形平凸翼形\n\n\n\n平衡状态分析：重心在c/4之后，才能静平衡\n\n第一张图：平凸翼形具有”低头“的总力矩（nose heavy moment），为了保持平衡，那么重心必须位于c/4之后，而具体这个距离的远近取绝于低头力矩的大小，也就是说，如果M*=0的话，那么我们需要刚好把重心放到c/4处。(带*表示平衡状态)\n\n静稳定性分析：不能静稳定\n\n第二张图：考虑受到扰动抬头的情况，升力将会增大，然而总力矩几乎不变，因此平衡将被打破，机翼将绕中心顺时针旋转，也就是会”抬头“。这时候，这个系统是静不稳定的，我们就需要引入尾翼来进一步使其具有静稳定性了。\n\n\n\n弯折翼型\n\n\n\n\n平衡状态分析：重心在c/4之前，才能静平衡\n\n第一张图：弯折翼形具有”抬头“的总力矩（tail heavy moment），为了保持平衡，那么重心必须位于c/4之前，而具体这个距离的远近取绝于低头力矩的大小，也就是说，如果M*=0的话，那么我们需要刚好把重心放到c/4处。(带*表示平衡状态)\n\n静稳定性分析：可以静稳定\n\n第二张图：考虑受到扰动抬头的情况，升力将会增大，然而总力矩几乎不变，因此平衡将被打破，机翼将绕中心逆时针旋转，也就是会”低头“。这时候，刚好可以自动抵消掉受到扰动导致的抬头，所以这个系统是静稳定的。\n\n气动中心和稳定性\n气动中心：攻角改变，但是作用在这个点上的总力矩不变，这个点就是翼形的气动中心。大多数翼形的气动中心都在c/4附近。所以上面的分析中，使用了c/4这个点用来判断，这是因为在实际航模飞行中，c/4点比较好找！如果重心离气动中心远一些，那么飞机的静稳定裕度就越大，静稳定性越强。\n后掠翼设计\n为了使得飞机在纵向具有一定的静稳定裕度，我们熟知的一个要点是重心(center of gravity, CG)要在气动中心(aerodynamic center, AC)之前。对于飞翼飞机而言，采用后掠翼的一个好处是其能够有效将气动中心后移，从而使得重心能够更方便地配平。\n\n平直翼与后掠翼的对比\n气动中心和稳定性\n\n平均气动弦长Cm计算\n\n\n\n\nlu:平均气动弦长；lr：翼根部弦长，λ=lt/lr.梢根比\n\n\n后掠翼飞翼气动中心的位置\n\n\n\n\n​\t如图，我们能够找到平均气动弦长在机翼上的位置，通过这个位置我们就能够确定气动中心的位置，然后将重心设计到这一点前面即可使得飞机具有静稳定性。\n​\t除了以上基于几何关系的方法，我们还能采用下列的经验公式直接估算气动重心的位置。\n\n机翼的扭转（twist）的设计\n\nCG处在AC之前是纵向稳定性的需求，除此之外，为了平衡性，在CG附近的力矩和必须为零。首先因为我们已经确定了确保纵向稳定性的CG位置，我们只能通过调整机翼的扭转使得气动力矩在CG附近的和为零。\n在常规布局的飞机上，我们可以通过调整水平尾翼来调整这个力矩，但是在飞翼上我们无法这样做。并且计算这个扭转是十分复杂的\n这里参考国外给出的一种估计扭转角的大致方法，这个方法主要借助两张经验图，它可以适用于根部到梢部线性变换的后掠翼。\n机翼需要的扭转可以分为两部分：几何扭转和气动扭转\n\n几何扭转：翼根和翼梢翼形的x轴的夹角，一般翼尖为负扭转；大的扭转角可以用来稳定具有小后掠角或者高弧度的翼形，但是缺点是当机翼工作在超出设定范围时会增加诱导阻力。\n气动扭转：翼根和翼尖采用不同的翼形。如果我们选择零升攻角不同的翼形，我们可以减小几何扭转量。这样的话可以减少几何扭转量从而使得机翼的设计性能更好。\n\n\n\n计算几何扭转角βreq\n\n我们可以通过两张图表来确定需求的扭转角βreq.\n\n\n这张表是\n\n\n根据CL*=1.0，\n\n\n稳定系数σ*=10%\n\n\n使用零力矩系数的翼形\n\n\n得到的标准情况，我们可以将自己的翼形设计的比例叠加上去就能得到我们实际需要的βreq，其中我们可以发现，如果CL=0.5,即缩小50%，那么机翼的扭转也能缩小50%；除此之外，要是我们采用较小的稳定系数，那么也能减少机翼的扭转\n零升攻角的变化\n如果我们在翼根和翼梢采用不同的翼形，那么他们的零攻角升力方向可能不一样，这将影响平衡状态。通过这种手段，我们可以减少几何扭转\n\n如果翼根，翼梢采用相同的翼形，那么这个角就是0.\n翼形力矩系数的影响\n\n翼形的力矩系数对于平衡也有影响，上面这张图可以用来帮助我们找到对扭转角进行修正的合适角度。如果我们采用了具有正力矩系数的翼形，那么β_cm就会为正，那么它将能够减少扭转量。这张图也是根据标准翼形的参考所描绘的，其中cm*=0.05。如果我们在翼根和翼尖采用了不同翼形，那么我们需要利用(c_m,tip+c_m,root)/2来计算这个力矩系数。\n最后，我们可以计算出机翼几何扭转角的净值β_geo,即\n\n","categories":["Aircraft Design"],"tags":["design","fly wing"]},{"title":"垂直起降倾转无人机-我的cadc记录","url":"/2020/10/25/%E5%9E%82%E7%9B%B4%E8%B5%B7%E9%99%8D%E5%80%BE%E8%BD%AC%E6%97%A0%E4%BA%BA%E6%9C%BA-%E6%88%91%E7%9A%84cadc%E8%AE%B0%E5%BD%95/","content":"引言\n这个故事开始于我大四上学期的深秋，它的起点是很低落的。那时候我感觉自己的大学生活已经彻彻底底地“失败了”，“结束了”。很多生活的不如意烦恼着我，平均每两天我就得去游一次泳，因为只有游泳的时候感觉自己很自由很快乐，这几乎成为了我续命的事务。\n一次游泳结束后，走向食堂的小路上，看着周边的夕阳，回想自己在这个学院学习的几年时光，让我很难以释怀的诸多事情之一就是自己作为一个工科学生，虽然锻炼了很多工程能力，但是没有一个机会让我真实地展示出来。从大一的车载地面站，到大二上的载人机，到扑翼机。这些项目大多做到一半就因为人事原因和其他不可抗因素而终止了，实在是很可惜。 我也想看到自己造的飞机成功地飞一把啊！于是我看到cadc有关消息就去报名参加比赛了。\ncadc对于我一个已经大四上的人来说，基本没什么作用了。即然一切都已无法改变，那还不如珍惜这点时间做点自己开心的事情把，开始干吧！\n\n这些便是写在前面的话，也是我的初心把。后面的内容主要是偏技术经验总结，但是我为什么要把这个放在最前面呢？那便是想说明：如果你也参加cadc，不管怎么样，希望你不是为了最后那个小小的奖杯，因为她真的不是一个 产出/投入 很有优势的比赛，在最后的比赛上成功地飞出来固然很好，但是还有很多队伍在最后环节一个失误便会葬送他们前面所有的日日夜夜的努力，所以，想要得奖，想要刷简历，想当人上人，不要选择cadc，但是如果，你也能放下一些成败心，只想看到自己的飞机畅快地飞行，看到自己的智慧在自己的飞机的设计上闪着光，那么十分欢迎你继续往下阅读！\n\n垂起比赛要求\n2020年的垂起比赛要求其实挺简单。限制两个电机，一块80g以内的动力电池，需要飞行器进行多次装载、绕飞、投球的过程。3min之内，谁投的多，投的准，谁的分数就更高，另外，没有降落分！\n构型确定\n由于这是SJTU第一次参加这个比赛，我们也没有什么经验。所以我们仔细观看了往届各个参赛队伍的参赛视频，然后选出了：具有可行性+飞行性能不错的设计形式，主要有：\n\n\n圆筒机身固定翼吊机方案\n\n\n横列式倾转双旋翼\n\n\n纵列式倾转双旋翼\n\n\n其中，第一种方案对于飞手要求太高，我们第一年参赛，疫情还占用了一个学期的备赛时间，飞手无法进行十分长期稳定的训练，所以我们首先放弃；第二种方案的控制难度太大，需要研究垂起转平飞过程的飞行控制，我们没有太多的信心能完全突破，所以也放弃了；所以我们选择了第三种方案——纵列式倾转双旋翼。\n总体介绍\n不多说废话了，上图！\n\n纵列式倾转双旋翼概念图\n控制方式：\n\n\nroll : 舵机同向倾转\npicth : 电机差速\nyaw : 舵机反向倾转\n\n\n垂直转平飞转换：\n其实这个过程是**“伪转平飞”**，固定翼模式只需要电机保持较大的差速，使得机身倾转角度变大同时前飞，然后使得机翼提供一部分升力即可。\n分系统介绍\n结构设计\n气动设计\n控制系统\n混控设置\nPID调试\n满/空载PID切换\n推进系统\n","categories":["Life"],"tags":["life","VTOL","drones","cadc"]},{"title":"雷达-相机信息融合的一些问题及解决方法（2019-2020.4）","url":"/2021/04/28/%E5%BC%82%E7%B1%BB%E4%BC%A0%E6%84%9F%E5%99%A8%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88%E6%80%BB%E7%BB%93/","content":"应对雷达信息的稀疏性\n​\t雷达点云数据具有稀疏性，而图像数据是密集的，这使得融合难以有效进行。先前的学者通过柱扩张的方式强行使得雷达数据更加稠密，这虽然有一定的效果，但是还是难以起到很好的说服力。\n柱扩张及其变式\n​\t这类方法主要针对点云数据，他们都认为点云代表了目标的位置，用柱体扩张来代表目标的体积，然后投影到像平面上进行进一步的融合。这里面有**“以点代面”**的思想\n2019 - Felix - CRF-Net\n\n​\t这种方法是比较原始的，最早还是Felix于19年使用，他的问题主要在于：\n\n\n对于不同类型的目标，柱体的大小应该不一样\n\n\n点云未必刚好在目标的中心，投影之后就存在偏差，甚至未必对应目标（其实是配准问题）\n\n\n2021 - Nabati - CenterFusion\n​\t在WACV2021中，Nabati巧妙地将检测框和柱体扩张通过视锥关联机制整合到一起，不再是简单地投影柱体，而是将检测框缩比，解决了上面两个问题。\n\n改变雷达数据形式\n​\t改变雷达数据的形式，用频率图、时频图等形式。受数据集的限制，（2019-2020年主要还是基于nuScene和KITTI，他们两个都是只有雷达点云数据）直到2021年具有更底层雷达数据形式的数据集才陆续出现，以下两者都自己采集了数据集。\n2021 - Wang Yizhou - RODNet\n\n\n\n使用了雷达频率图RF\n\n\nRAMaps(range-azimuth heatmaps)的性质:\n\n可以被解释为在BEV下，x轴为角度，y轴为距离\n雷达回波信号→FFT→LPF（low-pass filter），然后从另外一个接收天线也进行FFT从而估计角度，最后得到RAMaps\n\n\n\nRF data的性质：\n\n\n丰富的运动信息。为了利用这个运动信息，需要考虑多个连续的雷达帧作为输入\n\n\n不一致的分辨率：距离分辨率高，角度分辨率低\n\n\n不同的表示形式\n\n\n\n\n2021 - Wang Zhangjing - RCF-Fast RCNN\n使用具有微多普勒特征的雷达时频图像。\n\n微动与微多普勒特征：通常定义微动(Micro．Motion)为除质心匀速以外目标的微小运动，这种定义忽略 了径向加速带来的影响。目标的径向机动也会使信号频谱展宽，所以除了是质心作匀速 运动以外的运动，如振动、自转，旋动，翻滚、加速运动等都可以称为微动。从多普勒效应上来看，目标的微动会在频谱上引起额外的调制，出现旁瓣或者频谱展宽等，这种现象就称为微多普勒效应\n\n\n\n挖掘雷达数据的时序信息\n​\t挖掘雷达的时序信息是一个目前还关注十分少的点，这种方法具有一定的合理性，原因在于\n\n传统的雷达识别方法，例如RCS法就需要时序雷达信号信息\n考虑到时序性之后，有助于缓解雷达信号的稀疏性\n考虑时序信息之后，随着时间的增加，对判断的结果的置信度也能不断提升\n\n2021 - Felix - KPConv-CLSTM\n​\tFelix最先注意到了相关内容，但是受限于只有点云雷达数据，他利用核点卷积层（Kernel Point Convolution ，KPConv ）来提升特征提取效果。 他使用了LSTM长短时记忆神经网络来学习时序雷达信号的特征。\n\n\n雷达-相机信息数据配准\n​\t2019年，Felix提出了经典的融合方法，将雷达数据点进行了投影，对两种数据都分别通过多层VGG16网络提取特征之后，让雷达和图像数据自动学习出融合的深度是多少，然后进行concate，但是最终的效果却并不是很好。\n​\t之后的学者发现提升融合精度的主要关键在于：数据配准。即：针对同一个目标，需要将有效的雷达数据和图像检测框进行关联\n​\t这个问题的难点主要在于：\n\n\n雷达检测和图片检测很可能不是一对一映射的；很多场景中的目标会产生很多个雷达检测，并且也有些雷达检测根本不对应任何目标。\n\n\n雷达的z方向并不准确，甚至根本不存在：由于雷达的z方向不是十分准确（甚至根本不存在），映射后的雷达检测可能会超出相应目标的2D检测框。\n\n\n被阻挡的物体如何关联？（有雷达点, 无检测框）：被阻挡的物体会映射到图片的相同区域，这使得在2D图像上区分他们十分困难，几乎不可能。\n其实，从逻辑上来讲，雷达和相机都只能得到目标的某些特征，配准更像是用\n\n\n视锥关联机制\n2021 - Nabati - CenterFusion\n\n基于表征学习的关联方法\n2021 - Xu Dong - RCF-Representation learning\n\nAssociationNet原理：利用AssociationNet学习每个雷达针和每个检测框的语义表征信息。在这个表征下，一对匹配的雷达针和检测框将会”看上去“相似，即他们的学习表征距离比较接近。\n\n\nBackbone：ResNet\nFPN：融合不同尺度的特征\ntwo extra layers ：将特征恢复到输入图的尺寸\n\n输出的特征图包括了雷达针和检测框的高维语义表征信息。每个雷达针和检测框都在特征图中有独有的像素位置，我们在输出的特征图的对应像素位置提取他们的表征向量。这个过程就是process c。\n\n\n推理使用过程\n\n\n\n\n","categories":["Paper Reading"],"tags":["sensor fusion","radar"]},{"title":"2021 - 组会汇报记录","url":"/2021/03/25/%E6%AF%8F%E5%91%A8%E5%B7%A5%E4%BD%9C%E6%B1%87%E6%8A%A5/","content":"3月\n3.4-3.11\n\n尝试了一下dji-tello的sdk二次开发。目前已经实现的功能：实时图像回传(720p,30fps),实时控制移动，飞行器状态回传。（基于py3.7）\n\n\n\n\n\n\n\n\n学习了一下python多线程。（为解决目前移动过程中图像卡顿问题\n\n3.11-3.18\n\n调试演示程序，和张世钊师兄，帅欣成进行识别跟踪算法部署\n\n\n3.18-3.25\n\n\n重新拍数据集改进演示程序识别效果; 增加电量、温度、x,y,z速度等信息回传。\n\n\n\n\n\n\n\n阅读了两篇论文，并完成一次小组会论文汇报：\n\n\nCenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection\n\n\nKernel Point Convolution LSTM Networks for Radar Point Cloud Segmentation\n\n\n\n\n完成最优估计作业：滤波估计仿真\n\n\n\n\n\n\n\n3.25-4.1\n\n科研：\n\n精读了一篇有关异类信息融合（特征级）的论文：Radar Camera Fusion via Representation Learning in Autonomous Driving （Pre-published，2021，Xudong）\n调研了一下21年以来异类信息融合（特征级，雷达相关的）的重要论文\n\n自学Pytorch\n\n\n课程：\n\n完成现代飞行控制系统课程大作业\n\n\n\n项目：\n\n调研了一下有关机载雷达相关知识，了解了一些有关合成孔径雷达成像原理，分辨率等。主要来自：Stimson’s Introduction to Airborne Radar, 2014, 3 Edition\n\n\n\n\n4月\n4.1-4.8\n\n科研：\n\n\n\n精读了一篇异类融合相关的论文（使用了雷达频率图信号，跨模式监督学习，WACV2021）：\nRODNet: Radar Object Detection using Cross-Modal Supervision\n\n\n受老师委托，建立了一个小组内论文笔记交流学习的平台：https://saacv.cn/\n\n\n\n\n\n4.8-4.15\n\n\n课程学习\n\n\n项目：\n\n\n\nH863：制作演示程序QT软件界面（目前正在解决实时视频流播放问题）\n十所：和齐宇轩确定了空空目标所需属性，调查了目标机动与意图的对应关系、基于目标机动的态势计算。\n学习了一下simulink的基于flightgear的可视化。\n\n4.15-4.22\n\n项目：\n\n\n\n十所：\n\n\n和齐宇轩确定贝叶斯网络的训练数据\n\n\n使用unity进行态势威胁仿真（宋义仁协助），目前实现了\n\n\n目标欧拉角、位置的输出\n\n\n威胁系数模型的计算（目前还只考虑了角度优势和攻击有效性）\n\n\n目标的框选显示\n\n\n\n\n\n\n\n\n截图画面解释：\n\n\n绿色框中的是敌人，第一排数字（敌人位置）；第二排数字（敌人欧拉角）第三排数字：威胁指数。\n\n\n红色三角：高威胁目标（目前设置为威胁指数&gt;0.05）\n\n\n\n\n后续要进行的改进\n\n进一步优化威胁模型（距离，速度优势模型）\n更改飞机和战场模型（换为现代飞机和我们所需战场）\n敌机的运动状态设置\n整合入意图推断模型\n设置灵活的scene视角机制\n\n\n\n\n科研，精读了两篇论文(雷达，相机异类特征级融合)\n\n\n\nRRPN: Radar region Proposal network for object detection in autonomous vehicles(IEEE ICIP)\n\n\nResearch of Target Detection and Classification Techniques Using Millimeter-Wave Radar and Vision Sensors(Remote Sensoring)\n\n\n","categories":["Weekly Work"],"tags":["weekly work"]},{"title":"空空战场态势感知-意图与威胁判定","url":"/2021/04/09/%E7%A9%BA%E7%A9%BA%E6%88%98%E5%9C%BA%E6%80%81%E5%8A%BF%E6%84%9F%E7%9F%A5-%E6%84%8F%E5%9B%BE%E4%B8%8E%E5%A8%81%E8%83%81%E5%88%A4%E5%AE%9A/","content":"1. 空中目标属性\n数值属性\n​\t方位角（单位：mil：千分之一弧度）\n​\t距离\n​\t俯仰角\n​\t速度\n​\t高度\n\n非数值属性\n​\t在实证应用方面，根据雷达站空战数据的实际特点，构建了空战目标作战强度数据库。\n​\t空中目标的信息数据包括数值特征数据和非数值特征数据两部分。数值特征数据包括空中目标相对于我方的方位角、距离、航向角、速度和高度。非数值特征数据包括空对空雷达状态、空对地雷达状态、电子干扰状态、雷达横截面水平等。\n​\t空对空雷达状态\n\n\n1：目标正在使用雷达搜索空对空目标或引导空对空导弹\n\n\n0：没有上述状态\n\n\n​\t空对地雷达状态\n\n\n1：目标正在搜索地面和海面\n\n\n0：没有上述状态\n\n\n​\t电子干扰状态\n\n\n1：指目标正在使用相关设备进行电子干扰任务。\n\n\n0：没有上述状态\n\n\n​\tRCS等级\n\n1：隐身飞机，RCS&lt;1\n2：中型飞机，1&lt;RCS&lt;10\n3：大型飞机，RCS&gt;10\n\n\n2. 目标意图关系表\n\n3. 基于目标机动态势威胁建模\n​\t现有威胁评估模型主要针对 3 代常规作战飞机， 态势威胁建模主要从角度优势、距离优势、速度优势 3 个方面对空中态势进行评估。这里的模型还对隐身飞机的情况进行了修正。\n探测有效性因子\n​\t在态势威胁计算中， 若只考虑双方的相对态势，则实际上隐含了一个假设， 即双方可以完全感知对方。但在隐身条件下， 此假设己经不再适用。隐身作战条件下， 载机具备较强的信息隐蔽能力， 受低可探测性和己方先进电子战系统的支援， 有可能导致空域目标无法准确感知， 从而无法形成有效的战术策略， 因而从某种意义上降低了威胁的“有效性”。文中提出探测有效性因子衡量空域目标探测威胁的有效程度:\n \n式中: Pa 为当前态势下目标对我方的探测概率; k 为修正因子， 用以修正探测概率对威胁有效程度的影响。\n攻击有效性因子\n​\t受雷达探测能力等因素的影响， 隐身条件下的导弹发射距离也受到限制。空战对抗中， 敌导弹攻击是主要威胁， 可用我机与敌导弹攻击范围的相对位置来衡量攻击威胁。定义攻击有效性因子表示敌导弹攻击范围对态势的影响:\n\n​\t式中: ＲMmax为导弹最大发射距离; ＲKmax 为导弹不可逃逸攻击距离; φ∈［ － π， π］ 为目标进入角。\n角度优势模型\n\nϕ , φ 分别是我机速度和敌机速度与目标线的夹角， 0&lt; | | ϕ , | | φ &lt;180。\n\n距离优势模型\n​\t影响距离威胁指数的主要因素包括目标空空导弹的最大攻击距离 Rm-max 、最小攻击距离 Rm-min 、最大不可逃避区距离 Rnm-max 、最小不可逃避区距离 Rnm-min 和机载雷达探测距离Rr 。文献[9]的仿真结果表明：导弹对目标的杀伤概率在中间发射距离时较大，而在最近和最远发射区附近的杀伤概率较小且变化较快。在攻击区中间某一段距离发射时，杀伤概率大于某一设定门限值的区域称之为导弹的“不可逃避区”。\n​\t当距离目标机载雷达探测范围一定距离,R_a,pq (第 q 类目标对第 p 类目标构成威胁的情况)时,需开始关注目标，即目标由无威胁变为有威胁。构造距离威胁指数如下\n\n​\t上式中C1 、 C2 、 C3 、 C4 和C5 都是大于 0 且小于 1 的常数，其中 C1 + C2 + C3 =1。\n速度优势模型\n​\t当敌我速度分别为v_e 和ν_w 时， 速度威胁指数可表示为：\n\n态势威胁计算模型\n\n​\t式中: ρ 为探测有效性因子; λ 为攻击有效性因子;Td为距离优势; Ta 为角度优势; Th 为速度优势\n4.空中目标运动模型\n参考李晓榕: Survey of Maneuvering Target Tracking. Part I: Dynamic Models\n​\t在追踪民航客机时，2D模型通常就够用了，因为他们的转弯速率和纵向z周的变化通常都很小；但是在追踪具有高机动性的军用飞机目标时，由于他们具有“高g”特性，解耦的模型通常是不适用的。我们采用以下几种模型来描述他们：\n\n分别代表位置，速度，加速度\n代表体坐标系(body frame)，用来描述角度，目标的角速度被定义在体坐标系下，即\n\n","categories":["Summary"],"tags":["air combat","combat intension recognition"]},{"title":"网站建设记录","url":"/2020/10/23/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE%E8%AE%B0%E5%BD%95/","content":"网站建设记录\n2021.4.17\n\n\n主题升级为keep\n\n\n已支持数学公式\n测试：\n\n这是一个行间公式测试\n\n\n支持图片懒加载\n\n\n支持cdn加速\n\n\n支持评论系统，基于valine\n\n\n感谢XPoet，一直喜欢他的主题\n\n\n2020.10.22\n\n网站更新为hexo架构，感谢Easy Hexo的教程\n主题为ILS\nILS的极简设计风格能更好地突出内容\nmarkdown语言简直太棒了！一天上手，方便备份，轻量化\n基于github仓库，省出管理服务器的钱和精力\n一些原文章不便转换\n\n2019.4.24\n\n第一次尝试建站和接触服务器\n基于wordpress\n基于阿里云学生服务器\n\n","categories":["Life"],"tags":["life","website"]},{"title":"许志钦：理解神经网络训练过程","url":"/2021/11/16/%E9%A2%91%E7%8E%87%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"作者\n上海交通大学，许志钦：https://ins.sjtu.edu.cn/people/xuzhiqin/pub.html\nF-Principle\n频率原则可以用一句话概括：深度学习倾向于优先拟合目标函数的低频部分\n神经网络的特点：\n\nOverParameter：参数富集,能力上限很强，理论上可以拟合任何分布，也意味着它可以使用很多种参数组合来拟合相同的分布\n但是在它的能力中，具有较好泛化性能的参数组合只是一小部分\n\n\n\n\n为什么神经网络的学习效果比传统方法好？\n\n神经网络从低频开始匹配，传统方法从高频开始匹配\n\n\n\n\n输入数据的噪声同时存在于高、低频，但是数据的低频能量往往会比较大，所以在低频神经网络学习到了更多有用的信息，效果更好。\n\n\n\n\n高斯白噪声功率谱\n\n\n\n过拟合现象的解释：当不停地训练网络，它会开始倾向于学习一些高频信息，这时噪声的影响更大，神经网络的性能开始变差\n\n\n深度加速效应\n\n人们在运用中发现，更深层的神经网络往往比隐藏层较少的神经网络训练得快，也有更好的泛化性能\n\n\n\n越深的网络，达到某个固定精度的速度往往越快\n\nDeep Frequency Principle——更深层神经网络的有效目标函数在训练的过程中会更趋近于低频。\n","categories":["Summary"],"tags":["F-Principle - deep learning"]},{"title":"西瓜虫和蚂蚁的故事","url":"/2021/03/15/%E8%A5%BF%E7%93%9C%E8%99%AB%E5%92%8C%E8%9A%82%E8%9A%81%E7%9A%84%E6%95%85%E4%BA%8B/","content":"​\t在我还是一个小小小孩子的时候，天气好的午后，校本课老师会带我们到操场上玩，我们最爱做的事情就是不厌其烦地把一只只西瓜虫和小蚂蚁，从人工草皮下面抓出来。\n​\t西瓜虫喜欢生活在这种潮湿、阴暗、温热的地方，他们似乎觉得十分安全，全家老小相安无事，直到遇到了我们这群无聊的人类幼崽。他们被抓住的时候往往会跑一段，发现自己被发现后立马蜷成一个小团子，然后趁人不备的时候开足马力往草皮下跑。对于西瓜虫来说，这是一场不折不扣的灾难。他们当然也不会知道，我们抓住他们不是为了取食，甚至也不是为了杀死他们，仅仅是为了取乐。\n​\t蚂蚁可能相比于西瓜虫好一点，好在哪里呢？他们由于身材纤细，我们往往一不小心就能让他们解脱。所以相对于直接抓，我们更喜欢观察蚂蚁的各种反应。故意把他们的路截断他们会怎么样呢？把他们的回家的洞堵上，他们会迷路吗？你看，这只好像就在焦头烂额了！故意让他们找到食物，然后叫来一大群同伴，看他们搬一会在突然拿走，然后看看那知最先用触角传递信息的蚂蚁，他好像尴尬了！…额，怎么说呢，小时候做的事情，现在看起来确实挺狗的。。。\n​\t画风来到人类这边：一个阳光充足，风和日丽的午后，一群孩子来到了操场上，蚂蚁，西瓜虫这种小生物简直就是学习昆虫最好的教材了，孩子们也玩的不亦乐乎…一出人间悲剧与人间喜剧同时上演，令人感慨。\n​\t这时候你可能就会问了：你是动物保护主义者把？通过这种对比是圣母心泛滥，让我们为之忏悔？\n​\t非也。\n​\t针对这种现象，我想问一问，蚂蚁，西瓜虫这般弱小的生物，为何生存延续到了现在？你可能会说了，这个当然是自然选择咯，蚂蚁西瓜虫他们有自己的生态位置，所以如此这般的就活下来了，虽然他们单个个体的能力很弱，但是他们的繁衍速度很快…balba\n​\t那么蚂蚁会不会感觉难受？难道他们对于我们的这场“屠杀”一点反应都没有？你可能会说了，当然没有，因为蚂蚁连成型的大脑都没有，他们的神经元数量大概等于我们小腿上一段跳动的腿神经。他们连思考的能力都没有，怎么能感觉痛苦与悲悯这么高级的思维概念？\n​\t以前我感慨于进化对于个体的限制，就算你在蚂蚁、西瓜虫群体中，偶然发现了一大块食物，成为了“大英雄”；抑或是成为了一支蚁后，哺育了成千上万的后代，终归还是一个不足味道的虫子，你所做的这些努力，在进化的大潮中，对这个群体的贡献，可能0.000001%都不到…反过来回想我们人类自己，就能发现一个十分悲观、真实的现象：即时是最伟大的那些人，做出了很大很大的贡献，不也像是蚂蚁中那种优秀的个体一样微不足道吗？不管是第一个开始使用【火】使得人类摄入更多优质的蛋白质从而脑容量增大，还是发明了【造纸术】使得代际之间的信息交流效率提高数倍，还是导出了【质能方程】为核聚变获得超前的巨大能量指明道路的爱因斯坦，也许在玩弄着人类的“人类”面前，都是微不足道的。更可悲的是，我们绝大多数人，只不过碌碌无为，过完了极其极其平庸、无聊、卑微、苟且的一生。更更可悲的是，绝大多数人，终生连“生命的意义”这个问题，都根本没有思考过或者根本没有能力思考，他们就像是，一只蚂蚁，一只西瓜虫，一段跳动的腿神经罢了。\n​\t如果你看到了这里，可以不用先急着思考于反驳，基本上可以忘掉我上面说的那些了，因为我不是哲学专业的，以上基本上都是瞎想的，可以说漏洞百出，但是我想重新思考以下几个小问题：\n\n\n宗教神学为什么今天还存在？真的有必要存在吗？这些东西对于人类的发展的影响是正是负？\n\n\n人类真的能解放吗？解放后，我们还要不要劳动？如果不能最终解放的话，维持现状关注于分配财富不好吗？\n\n\n为什么要探索太空？等技术发育地更成熟，再快速探索不好吗？\n\n\n我这样一个人微言轻的作者，也不是哲学相关的专业学者，真的有必要花时间写这些漏洞百出的东西吗？这不是浪费我自己和读到这里的人的时间？\n\n\n… …\n下面我将给出一种以上问题的个人解答：\n所有的存在，所有的事情，所有的一切的一切，是的，都没有意义。\n但是所有的事情，都还是要去一件一件地做。\n\n\n​\t这个问题其实我真的思考了很久很久，直到最近从自己身上才发现了答案。且听我讲另外几个故事吧。\n​\t在我初中的时候，其实不太愿意给班级上一些同学讲题目，因为我觉得，他们的态度根本就不认真，我曾苦口婆心地给他们讲那些精彩的证明思想，他们根本毫无反应甚至根本不在意，我感觉他们只是为了得到最后那个他们确信的答案，那个从我口中得出的最后的【真相】，久而久之我就不愿意给他们讲了。但是另外一类同学其实学习很不好，然后也是偶尔问我题目，虽然我知道跟他们讲了他们很难听懂这些内容，但是我还是激情洋溢地讲解着，因为我发现他们真的很认真很努力地在倾听这些东西，他们成绩往往不如另外那类同学，以后也不会去好的高中或者上大学，但是我就是很喜欢跟他们讲，因为他们认真地学习，我觉得他们进步了，我的讲解是有意义的。他们听完往往会【自信】地感慨“啊，原来就是这样啊，很容易啊哈哈哈，走我们去玩吧！”，这时候我就会很开心地说：“是啊，就是这么简单，走走走，去玩吧！”\n​\t在我上大学后，来到这个专业开始我也很努力地学习，但是发现太多太多离谱的事情了，我无法理解。有的老师上课只是念ppt，有的同学抄袭别人的作业，另外一方面，学校和学院往往会对一些学生要求的问题遮遮掩掩，糊弄过去，这些都令我逐渐失望，但是看看几年之后我毕业了呢？这个时候才发现，我们这里出去的学生还是挺尴尬的，大概率就是到国企里面做一颗【螺丝钉】，所以大家就这样，心照不宣地生活着，我也陷入了迷茫所以大三大四开始渐渐思考这些问题，越来越迷茫…既然所有的事情都没有意义，那么为什么还要去做？\n​\t不知如何回答这个问题，还是回到原来的蚂蚁吧，在他们短暂的生命里，甚至不会有能力去思考“意义”这样的问题，他们只是日复一日，挖洞，搬运食物，雨天的时候，还得搬家到高的地方去，他们不知疲倦。他们在路上遇到了小时候的我，一个普普通通，充满好奇心的幼年小孩。只是一个幼年的人类个体，就可以对他们这个进化了数万年的物种的成年个体肆意蹂躏。。。但是他们还是继续不知疲倦地，继续工作。有石子把路挡住了就绕开，食物消失了赶紧告诉大家…\n​\t如今我长大了，在学习的书籍、文学作品中，逐渐了解到蚂蚁是一种被赋予【勤劳】的标签的个体，但我并不觉得蚂蚁对此有什么反应，这也只是人用来抒发诸如：我们要想蚂蚁一样勤劳这样的想法的工具。蚂蚁什么都没有做，蚂蚁这么几万年的进化，根本什么意义都没有啊！\n​\t但是我今天仔细想想，蚂蚁这样做着，总有一种【认真】的感觉。宗教有没有意义？没有，但是教徒们在认真地讲解；人类真的能终极解放吗？也许不能，但是有人在认真地追求着；毕全国之力登上月球或是取回月土，有什么意义？也许没有。但是那些聪明的头脑，正在认真地做着。当然这里的【认真】不是传统意义上那种一丝不苟，原谅我好像找不到什么更好的词汇，他类似于一种专注、态度、信仰、虔诚这样的感觉。我写到这里，说到底当然也是没有什么意义，但是我在【认真】地抒发我的思考，做着我认为有用、正确的事情，虽然根本没有什么意义。但是这种【认真】，值得各种维度上的尊敬。\n​\t剩下的，还是到生活中去思考把！\n","categories":["Life"],"tags":["Life"]},{"title":"A Deep Learning based Radar and Camera Sensor Fusion Architecture for Object Detection","url":"/2020/10/26/A-Deep-Learning-based-Radar-and-Camera-Sensor-Fusion-Architecture-for-Object-Detection/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\nAbstract\n利用深度学习的方法进行图像数据的目标检测在近些年来已经表现的非常成功了。不断提高的识别率和更高效的网络结构把这些技术推向了在量产车型上的应用。然而，图像传感器的质量受到恶劣天气条件的限制，并且传感器噪声会随着弱光环境和夜间环境而增大。我们的方法通过将相机数据和投影的稀疏雷达数据在网络层相互融合，强化了现有的2D目标检测网络。该CameraRadarFusionNet (CRF-Net)  能够在最有利于提升融合结构的融合层次进行自主学习。除此之外，我们采用了___BlackIn___,一种由Dropout提出的训练策略，它可以使得将学习过程聚焦到某一个具体的传感器类别上。我们的结果显示该融合网络可以在两种不同的数据集上性能超越目前最先进的只基于图像的网络。该研究的代码可以向公众开放，位于：https://github.com/TUMFTM/CameraRadarFusionNet\n\nwhy？：image-only的融合方法受到天气，环境光的影响大\nhow？:   神经网络 ：1. 融合雷达和相机数据 2. 自动学习融合层深度 3.BlackIn\nresult？：两种不同数据集上面成功超越现有image-only的网络\n\nIntroduction\nP1：问题引出\n雨天相机遮挡、弱光、夜间环境会影响相机感知效果，从而影响自动驾驶的安全性。\n\n\n\nP2：本文解决问题的方法\n为了解决这个问题，本文研究了用神经网络融合雷达和摄像机传感器数据的方法。该方法能够更可靠地检测nuScenes数据集和为本研究创建的TUM数据集中的目标。此外，我们还展示了我们的融合网络的局限性和未来的发展方向。\n\n雷达：直接获取目标的距离和径向速度信息。它能够在与地面平行的二维平面上定位物体。与相机相比，雷达传感器无法获得高度信息。\n\nP3：文章的内容结构\n\n第二节：讨论了目标检测和传感器融合的相关方法。\n第三节：描述了我们在将雷达数据融合到网络之前对其进行预处理的方法。\n第四节：继续描述网络架构。\n第五节：对该方法进行评估和讨论。\n第六节：结论\n\nRelated Work\nP1 ：卷积神经网络的发展\n\n[4]: 第一个成功将卷积神经网络应用于图像分类的公司\n[5][6]: 用于分类的神经网络结构被增强以执行额外的任务，如目标检测和语义分割\n[7]: 实际应用中，单镜头结构（single shot architectures）已经被证明在保持合理低计算时间的情况下执行准确\n[8]-[11]: 近年来，人们又提出了新的特征提取体系结构，将其应用于特定的元体系结构中，可以提高目标检测的性能。\n[12][13]: 最近的研究又出现了了自动调参（auto-tune）和初始神经网络结构设计，以增加检测性能或者减少运行时间，（在不明显影响检测性能表现的情况下）\n\nP2：激光雷达融合的发展\n\n\n[14]: 将激光雷达数据进行[^鸟类视觉投影]到2d地面，并且将相机数据与之结合以进行3d目标检测。\n\n\n[15]: 将激光雷达分别投影到地面和垂直图像平面，然后将两者和相机数据结合在一个神经网络中进行融合。\n\n\n[16]: 将激光雷达和相机数据在一个神经网络中进行融合来进行可行驶路段的分割。这些分支之间的互连是可训练的，这样在训练过程中，网络就可以在网络中学习到一个优化的深度级别，以便进行数据融合。\n\n\n[17]: 利用了和[16]相似的融合方式但是对于激光雷达和相机同时应用了[^鸟类视觉投影]\n\n\nP3-5：雷达融合的发展\n\n[18]: 使用雷达探测在相机图像中创建感兴趣的区域，以便用一个简单的神经网络在这些区域分类对象。\n[19]-[22]: 一系列利用雷达来得到目标检测的感兴趣区域的相似方法\n[24]: 融合每个传感器的独立跟踪检测，以产生一个包含两个传感器读数的最终位置估计。\n[25]: 提出了一种深度学习方法与生成对抗网络(GANs)融合相机数据和雷达数据，合并到一个2D鸟瞰图网格图，以执行自由空间检测(free space detection)。\n[26]:给出了传感器融合的深度学习方法的概览。他们指出原始数据层的相机数据和雷达数据很少被考虑，这方面的研究应该需要进一步进行。\n[27]:将低空雷达数据投影到垂直于道路的摄像机图像平面上，提出一种与摄像机图像融合的神经网络。他们使用距离和距离率的雷达作为额外的图像通道。 该论文提出了两种传感器的融合策略:\n\n一种是在初始分离层之后在固定层上进行合并(concatenation).\n一种是在初始分离层之后在固定层上进行元素方式的添加(element-wise addition).\n\n\n\nP6: 本文的工作及方法\n\n\n用了一个类似于[27]中的投影方法将雷达数据投影到相机图像的垂直面上当进行融合\n\n\n提出了一个融合网络，它可以使得融合结果最有利于减少网络损失的情况下进行学习。\n\n\n在图像空间中操作2D真实标注(ground truth)数据，与3D标签相比，这大大方便了训练数据的生成。\n\n\nP7：对运动目标滤波的说明\n\n由于有范围测量，通过雷达数据可以将运动的物体和环境区分开。对于某些实际应用，例如自适应巡航驾驶(Adaptive Cruise Control, AAC)，其对运动的物体进行了滤波以减少错误的雷达回波。同时，重要的静止物体，例如，停在红绿灯前面的车辆也被过滤了出来。在这篇文章中没有对运动目标进行滤波，所以我们可以同时检测到运动和静止的交通目标。\n\nRADAR DATA PREPROCESSING\nP1：雷达数据处理总述\n雷达数据的处理主要有四点问题：\n\n雷达和相机数据的空间校准\n如何应对雷达回波丢失的高度信息\n如何应对雷达数据的稀疏性\n用真实标注信息滤波(ground-truth filtering, GF)方法来消除雷达数据的噪声和杂波\n\nP2: 雷达和相机数据空间校准\n雷达输出的是具有雷达性质的2D-稀疏-点云信息，本工作中应用的雷达信息主要有方位角，距离，雷达散射截面(radar cross section, RCS)。我们将雷达信息从2D地面平面转换到了图像面的垂直面上。雷达回波信息被在扩维图像(augmented image)上面被保存为像素。无雷达回波的位置上，投影的雷达通道值的像素被标记为0。输入的相机图片信息包含三通道(红，绿，蓝)；我们将先前提到的雷达通道作为神经网络的输入。在我们的数据集中，三个雷达的the field of view(FOV)和前视鱼眼相机的FOV重合。我们将三个雷达点云信息合并到一个中去，然后利用这个作为投影雷达输入信息源。投影的算法不尽相同，例如nuScenes利用70°FOV相机，但是TUM利用180°FOV鱼眼相机。在nuScenes中，提供了摄像机内外映射矩阵，用于将点从世界坐标转换为图像坐标。而鱼眼透镜的非线性不能用线性矩阵运算来映射。我们使用[28]提供的校准方法将世界坐标映射到我们自己的图像坐标。\n总结重点：\n\n三个雷达点云信息合并为一个，作为雷达信息源\n雷达信息源投影到图像平面上\n鱼眼相机的世界坐标投影到图像坐标上\n投影雷达数据作为图像信息的扩维，无回波的地方像素就为0\n\nP3: 应对雷达高度信息缺失\n\n\n首先考虑到我们检测的目标主要是车，卡车，摩托车，自行车和行人\n\n\n我们假设了雷达得到的回波点的高度都是3m，以覆盖这些目标的高度\n\n\n然后这个高度线也作为像素投影到了图像平面\n\n\nP4: 应对雷达信息的稀疏性\n\nnuScene中激光雷达一次返回约14000点(相同水平张角下)[29], 这大概等同于雷达每周期探测57次的信息量.\n为了解决稀疏性问题,[25]利用了概率网格图(probabilistic grid maps)来获得雷达的连续性信息.\n我们通过融合最后13次雷达周期(约1s)的图像来提高雷达数据的密度.\n在这个方法中,我们补偿了自身运动(Ego-motion),但是目标车辆的运动无法补偿\n这种方法也增加了噪声,因为前一时刻对移动目标的检测与当前目标物体的位置不一致.但是为了增加额外信息,这个缺点是可以忍受的.\n下图展示了神经网络的输入数据的形式,雷达数据(距离和RCS)被标记在相同位置所以这里有均匀的颜色\n\n\nP5: 雷达数据的滤波\n雷达回波会返回需要与目标无关的检测信息,例如幽灵目标,无关目标和地面检测. 这些检测信息就是所说的噪声或杂波。在评价中，我们比较了融合原始雷达数据和利用两种额外滤波方法的雷达数据融合的效果。首先在nuScene中，只有一些标记目标被雷达检测到。在训练和估计中，我们因此实施了一种注解标注滤波器（annotation  filter，AF），这样经过滤波后的真实标注信息只包含至少同时被雷达点检测到的目标。这种方法对于可能被两种模式检测到的目标能发挥它的潜力。第二，我们采用了真实标注信息滤波器ground-truth filter来移除3D真实标注信息边界外的雷达探测点。当然，这个步骤如果实际场景是无法进行的。本文的目的是在输入信号杂波较少的情况下，证明融合概念的普遍可行性。经过雷达滤波后的图像在图像2b中被展示。注意，**GRF（ground-truth radar filter）**并没有输出完美的雷达数据，其中滤掉了部分数据的相关检测，原因有四：…（总结中有）\n\n总结：\n\n两种滤波器：\n\nAF:过滤没有同时被雷达数据检测到的目标\nGRF:过滤超出真实标注边界的目标\n\n\n滤波无法输出完美雷达数据的四点原因：\n\n没有对运动目标的补偿。雷达探测频率2Hz，探测周期中间的目标可能就消失了\n雷达数据和相机数据具有轻微的空间错校准\n雷达数据和相机数据不是刚好都是同一时间产生的\n虽然雷达距离测量非常可靠，但其测量并不完美，轻微的误差就会导致探测落在真实标注边界之外。在图2b中可以看到对部分相关数据进行了无意的过滤\n\n\n\nNETWORK FUSION ARCHITECTURE\nP1: 训练网络\n我们的神经网络架构建立在 RetinaNet[30]上 ，其中实现了一个[^VGG]的backbone[31][11]。这个网络被扩展以处理扩维图像的额外的雷达通道。网络的输出是边界盒(bounding box)的坐标系和分类分数的2D回归。该网络利用焦损失(focal loss)[30]进行训练。我们的基本方法是在第一个卷积层利用[^VGG]特征提取器\nP2: 雷达数据的特征提取和融合网络\n\n为什么不能直接融合（在第一层就开始融合）？\n\n\n雷达投影数据中的像素与图像数据中的像素意义是不同的，前者代表了目标的距离，对于驾驶任务更为相关。如果要直接融合这两种数据，我们需要假设他们是语义近似的；然而，由于上一点的原因，我们显然难以做出这种假设！\n\n\n那如何融合？\n\n\n在神经网络的更深层，输入数据被压缩为更密的形式，这种形式理想上包含所有相关的输入信息。即然我们无法确定两种传感器类型的信息的抽象层次，我们将网络设计为自主学习在哪种层次进行融合最有利于减少全局损失的形式。\n\n\n\n网络介绍\n\n相机和雷达数据在顶排被输送给该网络。\n在左侧，原始雷达数据通过[^max-pooling]来改变尺寸以送进更深的网络层。\n雷达数据被合并到前一级融合网络层的主要分支上\n特征金字塔网络(Feature Pyramid Network, FPN)[33],其中雷达数据被分级融合进去，通过将雷达通道合并到额外的通道上去\n最后FPN的输出通过回归和分类块进行处理[30]\n\n\n网络优势\n\n\n通过调节雷达特征在各个层的权值，优化器隐式地教会了该网络在何种深度将雷达数据进行融合具有最好的影响\n\n\n类似的技术被[16]所采用\n\n\n\n\nP3: 训练策略\n\nBlackIn :有意在部分训练中禁用了所有图像数据输入神经元，这个比例大概是0.2。这样做的目的是：图像数据的丢失能更加刺激该网络更依赖于雷达数据。‘教育’该网络稀疏的雷达数据是独立于密集的图像数据的。\n用预先在图像上训练的权重开始训练，用于特征提取器。\n\nEXPERIMENTS AND RESULTS\n数据集\n\n评价\n\n一个例子\n下图的对比能很好低表现CRF-Net的优势：\n\nCONCLUSIONS AND OUTLOOK\nP1：本文工作\n\n提出了CRF-Net结构来融合雷达和相机数据\n本研究适应了激光雷达和相机数据处理的思路，为雷达数据融合研究开辟了新的方向。\n对雷达数据处理的难点和解决方案进行了讨论\n引入了BlackIn训练策略进行雷达和相机数据的融合。\n证明了采用神经网络对雷达和摄像机数据进行融合，可以有效地提高目前最先进的目标检测网络的精度\n由于对雷达与相机数据的神经网络融合的研究是最近才开始的，寻找优化的网络架构还需要进一步的探索。\n\nP2：未来工作\n\n计划研究一种能滤除雷达数据中的噪声的预处理神经网络结构\n与其他传感器形式(如激光雷达数据)的融合可以进一步提高检测精度，但是同时会因为引入新层数和新设计概率而增加复杂性。\n神经网络融合方法对传感器空间和时间误校准的鲁棒性研究需要评估。\n我们看到了在恶劣天气条件下驾驶的多模态神经网络融合的增长潜力。需要创建具有这些条件的其他数据集来研究这一假设。\n最后，由于雷达传感器将距离信息引入到检测方案中，融合概念在三维目标检测中的适用性是我们想要探索的方向\n在硬件方面，高分辨率或成像雷达[38]有望增加雷达数据的信息密度，减少杂波。硬件方面的进步有望提高我们方法的检测结果\n\n附录-某些基础概念：\n由于我初接触神经网络相关知识，一些基本概念理解也附录上。\nmAP:\n\n\nmAP: mean Average Precision, 即各类别AP的平均值\n\n\nAP: PR曲线下面积，后文会详细讲解\n\n\nPR曲线: Precision-Recall曲线\n\n\nPrecision: TP / (TP + FP)\n\n\nRecall: TP / (TP + FN)\n\n\nTP: [^IoU]&gt;0.5的检测框数量（同一Ground Truth只计算一次）\n\n\nFP: [^IoU]&lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量\n\n\nFN: 没有检测到的GT的数量\n\n\nIoU：交并比（Intersection over Union）。[^IoU] 计算的是 “预测的边框” 和 “真实的边框” 的交集和并集的比值。\n\n\nground truth：\n在*有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth， 错误的标记则不是。（也有人将所有标注数据都叫做ground truth）\n来源：知乎：https://www.zhihu.com/question/22464082\nRetinaNet(Focal Loss)：\nRetinaNet由以下处理步骤组成提取、特征金字塔网络(FPN)和用于分类和回归的检测头。\nFocal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘。\n目标识别有两大经典结构:\n\n\n第一类是以Faster RCNN为代表的两级识别方法，这种结构的第一级专注于proposal的提取，第二级则对提取出的proposal进行分类和精确坐标回归。两级结构准确度较高，但因为第二级需要单独对每个proposal进行分类/回归，速度就打了折扣；\n\n\n第二类结构是以YOLO和SSD为代表的单级结构，它们摒弃了提取proposal的过程，只用一级就完成了识别/回归，虽然速度较快但准确率远远比不上两级结构。\n\n\n那有没有办法在单级结构中也能实现较高的准确度呢？Focal Loss就是要解决这个问题。\n参考内容：\nRetinaNet: Focal loss在目标检测网络中的应用\nRetinaNet(Focal Loss)\n[^VGG]:VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。下图是一个用3层3x3代替5x5的例子\n[^max-pooling]:max pooling 的操作如下图所示：整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出 output。\n[^IoU]:IoU 的全称为交并比（Intersection over Union），通过这个名称我们大概可以猜到 IoU 的计算方法。IoU 计算的是 “预测的边框” 和 “真实的边框” 的交集和并集的比值。\n","categories":["Paper Reading"],"tags":["sensor fusion","paper","research","object detection","Felix Nobis"]},{"title":"CenterFusion:Center-based Radar and Camera Fusion for 3D Object Detection | WACV 2021","url":"/2021/03/18/CenterFusion/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\nAbstract\n\n\n背景：目前提升检测和跟踪目标的鲁棒性和精确度的主要手段还是增加感知手段（传感器种类），这就使得传感器融合变为非常重要的一部分\n\n\n方法：CenterFusion\n\na center point detection network ,中心点检测网络\na novel frustum-based method  ，基于平截体视景的新方法\n\n\n\n结果：\n\n数据集：nuScenes\n比nuScenes Detection Score(NDS)上基于相机的最先进的方法分数提升12%\n进一步证明了CenterFusion在不使用任何额外时态信息的情况下显著提高了速度估计精度。\n\n\n\ncode: https://github.com/mrnabati/CenterFusion\n\n\n1.Introduction\nP1:传感器信息融合现状\n\n\n[4,10,14,19]: 2D和3D的目标检测\n\n\n[33,16]: 语义分割\n\n\n[1,7]: 目标跟踪\n\n\nP2:LiDAR+Camera融合\n近些年来，大多数采用：LiDAR + Camera 进行3D目标检测\n\nLiDAR：近距离准确，远距离点云稀疏，能力弱\nCamera：表面纹理信息丰富，深度信息差\n\n这种融合方式的限制\n\n\nCameras and LiDARs 都对恶劣天气敏感（雪、雾、雨）\n\n\nCameras and LiDARs  都需要使用(temporal information)时态信息才能估计物体的速度\n\n\n估计物体的速度在避免碰撞中，而需要时态信息在时序要求严格的情况(time-critical situations)下可能并不可行。\n\n\nP3:radar的特点\n\n成功应用在Advanced Driving Assistance System (ADAS) 和Adaptive Cruise Control(ACC)  系统上\n探测距离远（主动雷达可达到200m）、受恶劣天气影响小\n利用多普勒效应测速，不需要时序信息\n相比于LiDARs，radar只需要少量信息处理\n\nP4:radar融合少的原因\n\n数据集少\n应用于LiDARs的方法不好迁移到radar上\nradar点云更稀疏，不好提取3D信息。聚合多个雷达扫描会增加点的密度，但也会给系统带来延迟。\nradar点云虽然通常是在3d坐标下，但是其纵向测量往往不准确或根本不存在。所以大多数主动雷达只输出目标的距离和角度。\n\nP5:前/后/中层融合方法介绍\n[8]: 利用了神经网络层次特征进行融合\n（early fusion）前融合方法：将不同传感器的原始数据或者预处理数据进行融合，网络学习到了感知形式的一种联合表示。早期的融合方法受到空间和时序信息的误匹配很敏感。——数据层\n（late fusion）后融合方法：在决策层融合了来自不同感知形式的数据，并且网络可以灵活地加入新的感知形式数据。然而这种方式没有充分挖掘数据的全部潜力。——决策层\n（middle fusion）中融合方法：从不同的感知形式中提取出特征，然后融合特征，利用网络学习这些特征，并且在感知能力(sensitivity)和灵活性(flexibility)之间创造一种平衡。——特征层\nP6:我们的工作\nCenterFusion：聚焦于将雷达检测和图片的初步检测结果相关联，然后产生雷达特征图并且它和图片特征来准确地估计3D的目标检测框。\n算法步骤：\n\n1.利用关键点检测网络进行初步的3D目标检测\n2.利用新的基于视景的雷达关联方法将雷达检测结果和3D空间的目标进行关联\n3.然后将雷达的观测在图片中标出，用来创建特征图来完善基于图片的特征方法\n4.最后融合的特征被用来精确地估计目标的3D特性，例如深度，角度和速度。\n\n网络的结构如图1所示\n\n2.Related Work\n2.1 单传感器的方法\n单目相机3D目标检测方法（Monocular 3D object detection）:\n\n[11]：3D-RCNN: 利用额外的标签和3D投影使用R-CNN，还使用了一些列CAD模型来进行目标先验的形状类别区分。\n[17]：Deep3DBox首先使用CNN回归(regress)了一系列3D目标的性质，然后利用2D目标的检测框的集合约束来创建目标的3D约束框\n\nLiDAR 3D目标检测方法\n\n[12,35]: 基于3D体元（3D voxels）进行点云显示\n[13,5,29,31]: 基于2D投影来进行点云显示。\n\n体元方法通常速度比较慢，将得到的高纬度的3D体网格；投影法可能会受到目标在投影面上形状或体积的大的变化的影响。\n\n[25]：PointRCNN直接在原始点云上操作，使用点云分割，以自底向上的方式生成3D对象预选框(proposal)。这些预选框在第二阶段得到修正，以产生最终的检测框。\n\n2.2 基于融合的方法\nLiDAR和Camera融合\n\n[4]: MV3D从前视图和鸟类视角图(Bird’s Eye View, BEV)。LiDAR的BEV被用来生成3D目标预选框，然后一个深度融合网络被用来从每个视角融合特征来预测目标类和检测框。\n[28]: PointFusion分别使用CNN和PointNet处理图像和LiDAR数据，然后利用特征提取生成3D目标预选框。\n[23]: Frustum PointNet直接对从RGB-D相机上的原始点云数据进行处理，然后利用RGB图像和2D目标检测器来在点云图中定位目标。\n\nradar和其他传感器融合\n\n\n[30]: RadarNet将雷达和激光雷达数据进行融合进行3D目标检测，它利用一个前融合机制来学习两种传感器的联合表示，然后利用一个后融合机制来挖掘雷达的径向速度证据来提升目标速度的估计精度\n\n\n[3]: Chadwick等人将雷达结果投影到像平面然后利用他们来提升远距离目标的估计精度\n\n\n[20]: 作者首先使用雷达检测结果来生成3D目标检测的预选框，然后将他们投影到像平面进行联合2D目标检测和深度估计。\n\n\n[22]:CRF-Net也将radar检测投影到像平面，但是用竖线表示他们，其中像素值等于每个点的深度。图片数据然后被用雷达信息增广，并且利用卷积神经网络进行2D目标识别。\n\n\n3.Preliminary\n3.1 雷达点云\n主动雷达通常得到目标在BEV下的2D点云图，每次检测，雷达将会报告目标的径向瞬时速度。这个速度通常不是物体的实际速度，如图2所演示的。\n\n​\t我们将目标作为中心坐标系下的一个3D点来处理，P = （x,y,z,vx,vy），vx，vy分别表示了径向速度在x，y方向上的分量。径向速度已经通过车辆自身运动进行了补偿\n​\t对于每个场景，我们聚合了雷达点云的3次扫描(在过去0.25秒内的探测)。nuScenes数据集提供了从雷达坐标系统到自我中心和相机坐标系统映射雷达点云所需的校准参数。\n3.2 CenterNet\n​\tCenterNet代表了利用单相机的最先进的3D目标检测。\n\n输入：I:R(W×H×3)\n输出：关键点热图[0,1](W/R×H/R×C),R是下采样率，C为目标种类的个数，Y=1表示检测到在(x,y)处存在一个C类目标\n真值热图通过2D检测框真值通过高斯核生成\n\npi 是c类目标的真值中心，q是二维平面上的像素位置点，σi是尺寸自适应标准差，根据目标类别的大小控制热图的大小。然后一个全卷积编码-解码神经网络被用来预测Y。\n3D检测框生成：\n\n目标的深度、维度、方向直接通过探测的中心点分别回归得到。\n深度D[0,1](W/R×H/R)通过逆激活变换（inverse sigmoidal transformation  ）作为一个额外的输出通道输出。\n维度直接回归为三个输出通道，大小为其长宽高绝对值（单位：m）\n方向默认是单标量的值，然而其也很难回归。我们参考【A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka. 3d bounding box estimation using deep learning and geometry. In CVPR, 2017.】， 用两个bins来呈现方向，且i做n-bin回归。特别地，方向用8个标量值来编码的形式，每个bin有4个值。对于一个bin,两个值用作softmax分类，其余两个值回归到在每个bin中的角度。\n\n\n对于每个中心点，也预测一个局部偏移量来补偿骨干网中输出跨步造成的离散化误差\n损失函数：\n\n\n\n\n\nN：目标个数，Y：标注目标的真值热图，α、β焦损失的超参数。\n4.CenterFusion\n​\t利用[34]作为目标中心检测网络，返回目标的3D位置，方向和维度等属性。我们提出了一种中融合机制，它可以将雷达检测和他们对应的目标中心点关联起来并且挖掘雷达和图像的特征，通过重新估计他们的深度，速度，转角和分布来提升初步检测的效果。\n我们融合机制的核心是：精确地将雷达的探测结果和目标进行关联\n​\t目标中心点检测网络为每个目标类别生成一张热点图(heat map)，热点图的峰值代表每个目标的可能的中心，这些位置的图片的特征被用来估计目标的其他属性。为了在这个设置下挖掘雷达数据的特征，雷达特征需要被标记到图片中对应目标的中心，这需要对雷达检测结果和目标进行准确的关联。\n4.1 Center Point Detection\n​\t我们对于生成图片的初步检测采用了CenterNet检测网络，图片的特征首先利用一个全卷积编码解码骨干（backbone）网络。我们遵循CenterNet利用一个简化版的深层聚合网络（Deep Layer Aggregation，DLA）作为主干。提取出的图片特征然后被用来在图片上预测目标的中心点，以及目标的2D大小，中心偏移(Center offset)，3D维度，深度和转速。这些值都利用初步回归表(primary regression heads)估计。每个初步回归表包括256通道的3*3的卷积层和一个1*1的卷积层来产生预设的目标。这为每个 探测的目标提供了一个准确的2D检测框和初步的3D检测框。\n4.2 Radar Association\n​\t为了将雷达检测结果和相应的检测目标进行关联，一个naïve的方法就是：将雷达的检测点映射(mapping)到图片上，然后如果这个点在2D的检测框中就保留它。\n​\t这不是一种非常鲁棒性的方法，原因是：\n\n因为雷达的检测和图片中的目标很可能不是一对一映射的；很多场景中的目标会产生很多个雷达检测，并且也有些雷达检测根本不对应任何目标。\n除此之外，由于雷达的z方向不是十分准确（甚至根本不存在），映射后的雷达检测可能会超出相应目标的2D检测框。\n最后，被阻挡的物体会映射到图片的相同区域，这使得在2D图像上区分他们十分困难，几乎不可能。\n\n平截体视景关联机制\n我们开发了一种使用目标的2D检测框以及它的估计深度和尺寸来创建3D的感兴趣平截体区域（Region of Interest ，RoI）  的方法。在拥有目标的2D检测框的情况下，我们为目标创建了一个类似图3的平截体。\n\n这大大缩小了需要检查关联的雷达探测范围，因为任何超出这个截锥的点都可以忽略。然后，我们使用估计的对象深度、尺寸和转角来创建对象周围的RoI，以进一步过滤出与该对象不相关的雷达检测。如果还是有多个雷达检测在RoI内，我们选择最近的点作为这个目标对应的雷达检测结果。\n​\t在训练阶段中，我们使用目标的真实3D检测框来创建一个紧凑的ROI平截体并且将雷达检测和目标进行关联。在测试阶段中，ROI平截体通过上述的方法进行估计。在这个过程中，我们利用一个参数σ来控制RoI平截体的大小（如图3）这是为了考虑深度估计值的不准确性，因为在这一阶段，对象的深度完全是使用基于图像的特征来确定的.使用该参数扩大截锥，即使估计的深度稍有偏差，也会增加截锥内包含相应雷达探测的机会。σ的值应该谨慎选择，一个较大的RoI可能包含了其他临近目标的雷达检测结果。\n​\t截锥体方法的优点：\n\n\n解决了重叠目标的问题：RoI截锥方法使关联重叠对象变得轻而易举，因为对象在三维空间中是分开的，因此会有单独的RoI截锥。\n\n\n解决了雷达检测重复的问题：它还消除了多检测关联问题，因为只有在RoI截锥内最接近的雷达检测与目标相关联。\n\n\n​\t截锥体方法的不足：\n\n并不能帮助解决z方向的不准确问题，因为雷达检测到的高度信息不准确，可能在其对应对象的ROI截锥之外。\n\n柱体扩张\n为了解决高度信息不准确的问题，我们介绍了一种雷达信息预处理技术:柱体扩张，即每一个雷达检测点都被扩张为一个固定尺寸大小的柱体，如图4所示。柱体创造了雷达检测目标的一种良好的物理表现形式，拥有了这个形式，我们可以简单的认为如果一个雷达点的柱体都在截体内部，则认为这个雷达点在截体内部，如图1.\n\n雷达特征提取\n​    当将目标对应雷达数据完成关联后，我们就可以使用雷达的深度和速度数据来补全图片数据的特征。特别的，对于每一个关联到目标的雷达检测，我们根据检测框的位置生成三张热图（中心在目标的2D检测框处并且大小成比例），我们利用参数α来控制heatmap的大小。heatmap的值是归一化后的目标深度(d),和径向速度的x、y方向的分量vx，vy。\n\n\n\n特征图的编号：i = 1，2，3\n\n\n归一化因子：Mi\n\n\n特征值fi：（d，vx，vy）\n\n\ncx、cy：第j个目标的检测框中心坐标\n\n\nwj，hj：第j个检测框的宽和高\n\n\n如果两个目标具有重合的heatmap区域，那么具有较小深度的那个将主导，因为只有近处的目标在图片中完全可见。\n生成的热图将被合并到图片的特征中作为额外通道。这些特征将作为第二次分类回归器的输入来重新计算目标的深度和角度，以及速度和属性。第二次分类回归器包含三层3*3卷积核核1*1卷积核来产生理想的输出。这些额外的卷积层对比于第一次的回归分类器有利于学习来自雷达特征图的高维度特征。最后一步是将分类回归器的输出解码为3D检测框。\n5.Implementation Details\n\n\n目标检测网络：采用DLA为主干的CenterNet。（虽然Hourglass network为主干性能更好，但是采用了DLA因为其结果不错的同时训练速度快。）\n\n\nbaseline：直接采用发布版的CenterNet（已经在nuScenes上训练了140个epochs）这个模型默认不提供速度和属性，我们对于这两个head训练了30个epochs。利用这些结果作为我们的baseline。\n\n\n二次分类回归器训练：采用雷达和图片的特征数据训练了60个epochs，batch size：26.平台：2*Nvidia P5000 GPUs\n\n\n其他说明：\n\n\n像素降低：训练和测试过程中，图片像素从1600*900转为800*450\n\n\n数据增强-训练阶段：在训练期间我们使用数据增强，随机左右翻转(概率为0.5)和随机移动(从0到图像大小的20%)。同样的增强也适用于参考相机坐标系的雷达点云。我们不应用任何缩放增强，因为它改变了3D测量值。\n\n\n数据增强-测试阶段：在测试时，我们只使用翻转测试增强，将图像及其翻转版本输入网络，并将网络输出的平均值用于解码3D检测框。我们不像CenterNet那样使用多尺度测试增强。在[x, y, z]方向设置柱尺寸为[0.2,0.2,1.5]m， δ设置为将RoI截锥的长度增加20%。\n\n\n\n\n损失函数：除了对热图中心点头使用焦损失和对属性头采用二进制交叉熵(BCE)损失，我们对大多数回归头使用L1损失。\n\n\n6.Results\n\n\n7.Ablation Study\n\n\n\n相关知识\n1*1卷积层的作用\n神经网络可视化——heatmap\n","categories":["Paper Reading"],"tags":["sensor fusion","radar","object detection"]},{"title":"IVFuseNet Fusion of infrared and visible light images for depth prediction","url":"/2020/11/16/IVFuseNet%20Fusion%20of%20infrared%20and%20visible%20light%20images%20for%20depth%20prediction/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\nAbstract\n深度预测是无人驾驶研究的重要组成部分。现有的研究工作大多仅基于可见光图像或红外图像来预测深度。然而，可见光图像和红外图像都有各自的优势和劣势，在同一场景拍摄的图像中，这两种图像包含互补的信息。为了融合互补信息，预测不同条件下的深度，本文提出了一种基于卷积神经网络的深度预测体系结构，即红外和可见光图像融合网络IVFuseNet。具体而言，我们构建l 共同特征融合子网络、全特征融合子网络和高分辨率重建子网络，以充分利用这两种图像的互补性。共特征融合子网络采用双流(two stream)多层卷积结构，每层过滤器部分耦合，以融合红外图像和可见光图像提取的共同特征。全特征融合子网络通过自适应融合权值来融合共特征融合子网络生成的双流特征，而不是采用预设的融合权值。此外，高分辨率重建子网络采用残差密集卷积，将融合后的低分辨率特征精确映射到对应的高分辨率特征，增强深度预测细节的重建。所有三个子网协同进行深度预测任务。我们的NUST-SR数据集由无人驾驶车辆驾驶时捕获的实际道路场景组成。提出的IVFuseNet在该数据集上的性能最好。IVFuseNet将均方根误差减小到3.4513，平均相对误差减小到0.1651，优于其他方法。这个模型和数据集可以在这个找到：https://github.com/liyuqi1234/IVFN\nWhy？：目前的深度预测大多只基于红外或只基于可见光，融合这两者的互补信息能有效提高深度预测的精度\nHow？：共同特征子网络 + 全特征融合子网络 + 高分辨率重建子网络\nWhat？：在NUST-SR数据集上比其他方法都好\nIntroduction\nP1：基于视觉的自动驾驶简介\n\n[1-5]：基于视觉的自动驾驶方法在学术和工业界都吸引了大量的研究热情\n\n获得2维图像的深度信息能帮助理解场景中物体的几何关系，更进一步地强化了自动驾驶的安全性\nP2：基于可见光的深度预测\n\n\n[6-8]：大多数研究工作仅基于可见光图像来预测深度。传统的深度预测方法通常需要手动构造特征，如尺度不变特征变换(SIFT)和方向梯度直方图(HOG)。\n\n\n[9-13]：最近,基于卷积神经网络(CNN)的方法可以自动提取更有效的特征来预测深度并实现可观的收益\n\n\n[9]：提出了第一个利用可见光图像来预测深度的基于CNN的多尺度深度网络方法\n\n\n[14-15]：其他基于CNN的方法结合**条件随机场(condition random field  ，CRF)**来获得深度图\n\n\n[42]：Li等人利用**深度卷积神经网络(deep convolutional neural network, DCNN)**获取可见光图像的深度图，然后利用CRF对深度图进行细化。\n\n\nP3：红外图像的优点\n可见光进行深度预测受到场景限制大，深夜或者能见度低情况下很难提取出可靠特征。\n红外的优势：\n\n红外图像不受亮度变化的影响，在夜间光强变化和弱光条件下都能提取出稳定的特征\n红外图像具有很高的穿透力，能够穿透雨、雾、雪，因此解决了遮挡问题。\n红外图像的检测距离远高于可见光图像的检测距离。红外图像中非常遥远的物体也是清晰的。\n\nP4：基于红外的深度预测\n\n[16-19]：基于红外的方法\n[16]：Xiao等人基于多尺度和空间上下文信息提取红外图像的特征，弥补红外图像局部信息不足，通过**独立分量分析(independent component analysis, ICA)选择适合红外图像深度预测的特征，然后通过非线性支持向量回归(nonlinear support vector regression, SVR)**预测深度。\n[18]：Sun等人提出了一种基于核主成分分析(KPCA)和全连接神经网络的方法。在他们的工作中，利用核主成分分析提取适合红外图像的非线性特征，然后利用这些特征和对应的地面真值深度训练BP神经网络。\n\nP5：红外图像的不足及融合的好处\n虽然红外图像在某些方面弥补了可见光图像的不足，但由于**红外图像缺乏纹理信息，对比度较低，仅基于红外图像进行深度预测对于小目标和目标边缘的预测效果不够准确。**可见光图像纹理信息丰富，对比度高，弥补了红外图像的不足。通过以上分析，基于红外和可见光图像融合的深度预测可能比单纯基于可见光图像或红外图像的深度预测具有更好的性能\nP6：红外与可见光的融合\n\n[41-42]：重要问题：两种图像融合的配准\n[20-27]：一些成功的图像融合方法\n[20]：使用了拉普拉斯金字塔 Laplacian pyramid\n[21]：小波变换 wavelet transform\n[22-24,27]：基于CNN的方法来融合不同种类的图像\n[24]：Prabhakar等人提出了极值曝光图像对的曝光融合，CNN用于融合曝光图像对的亮度通道，加权融合策略用于融合输入图像的色度通道。\n\nP7:融合的优势及先前融合方法的问题\n\n光学相机-红外融合的优势：\n\n红外：更能预测弱光场景的深度\n光学相机：光照充足，物体纹理清晰的时候深度预测更好\n\n\n先前工作的问题：\n\n先前工作采用等权值或者手动调整的权值进行融合，忽略了两种图像在不同场景中深度预测的贡献不同的问题\n\n\n\nP8-9 ：本文的主要工作工作及章节介绍\n在本文中，我们提出了一种基于CNN的架构，称为IVFuseNet(图1)，自适应融合红外和可见光图像的互补信息，进行深度预测，它具有：共特征融合子网络、全特征融合子网络和高分辨率重建子网络3个子网络。\n\n共特征融合子网络：\n\n\n部分耦合过滤器 : 为了保持显著特征，我们在共特征融合子网络中提出部分耦合过滤器，在自适应加权融合前进一步增强特征。耦合过滤器有助于发现更有鉴别性的特征，这些特征很容易被彼此忽略。\n\n\n\n全特征融合子网络\n\n\n\n自适应加权融合策略： 首先，我们考虑了红外图像和可见光图像在不同场景下对深度预测的贡献不同。提出了自适应加权融合策略，其中系数矩阵表示两种图像的不同贡献。\n\n\n高分辨率重建子网络：\n\n\n残差密集卷积 RDC : 此外，融合特征的分辨率在卷积过程中降低，小于ground truth深度图像的分辨率。在深度预测之前，我们需要恢复融合特征的分辨率。受[28,29]的启发，**残差密集卷积(residual dense convolution, RDC)**在高分辨率重建中具有良好的性能。因此在高分辨率重建子网络中，我们在高分辨率重建子网络中采用RDC，也可以增强融合的特征。\n\n\n我们的主要贡献如下：\n\n\n在共特征融合子网络中设计了部分耦合过滤器，从红外图像和可见光图像中提取特征，学习红外图像和可见光图像之间的可转移特征。这相当于在子网络学习过程中，保留两类输入图像的单个特征，同时融合共同特征。\n\n\n自适应加权融合方法考虑了红外和可见光图像在不同场景下的不同贡献，并通过全特征融合子网络训练自适应融合系数矩阵，而不需要预先设置人工加权。\n\n\n将残差密集卷积块用于高分辨率子网络中进行深度预测，这些残差密集卷积块有助于恢复细节，提高特征融合效果。高分辨率重建子网络将低分辨率特征精确映射到相应的高分辨率特征，用于监督学习。\n\n\nRelated Work\n与我们的工作直接相关的是一些预测深度和融合红外和可见光图像的方法。基于CNN的方法已经成为深度预测最重要的方法。\nP1：基于CNN的深度预测方法\n\n\n[10]：Eigen等人提出了一个三尺度的卷积网络来预测深度。在他们的工作中，将每个尺度的输出预测传递给下一个尺度。采用三种尺度逐步细化预测深度，提取更多的图像细节。\n\n\n[19]：Gu等人设计了二维残差神经网络和三维CNN组合网络，该网络结合光流信息考虑连续两帧图像之间的信息。\n\n\n[30]：不使用全连通层，Laina等人使用全卷积残差网络预测深度，并提出了一种新的上采样块来提高输出分辨率，\n\n\n这些方法很难充分利用原始低分辨率特征的所有层次信息。受**[28,29]**的启发，我们在我们的网络中采用了残差密集卷积块。残差密集卷积块允许从前一层的输出直接连接到当前层，以利用所有层次特征。\nP2：可见光与红外的融合\n融合可见光图像与红外图像的互补信息在图像处理领域得到了广泛的研究[\n\n\n[25-27,31]：可见光与红外融合\n\n\n[22]：Li等人设计了一个DenseFuse网络，从源图像中获取更有意义的特征，并将特征进行等权融合。\n\n\n[27]：Ma等人[27]提出了一种生成式对抗网络FusionGAN，用于融合红外和可见光图像。发生器产生一幅融合了红外信息和附加可见信息的图像。鉴别器使融合后的图像具有更多的可见光图像纹理信息。这些方法主要侧重于特征提取，而不同特征的融合则简单地通过预先设定的权重进行。但是，两种图像的权重在不同光照条件下是可变的，以进行深度预测。\n\n\nP3：我们的方法\n与上述融合方法不同的是，我们首先设计了一种具有部分耦合过滤器的共同特征融合子网络来融合共同特征。其次，采用自适应加权融合策略融合共特征融合子网络中的全特征，使红外图像在弱光条件下贡献更大，可见光图像在充足光条件下贡献更大，从而实现深度预测;\nTechnical approach\n在本节中，我们将介绍用于深度预测的红外和可见光图像融合网络(IVFuseNet)。我们的网络示意图如图2所示。在共特征融合子网络中设计部分耦合过滤器，提取浅层特征，融合两种不同类型图像的共同特征。在全特征融合子网络中，我们提出了一种自适应加权融合策略来融合共特征融合子网络提取的全特征。随后，我们设计一个高分辨率重建子网络来增强地物细节和预测高分辨率深度地图。\n共同特征融合子网络\nP1 ：共同特征和个体特征\n如前所述，从单一RGB摄像机获取的图像可能包含的信息不足，无法在特定光照条件下提取深度预测。在提出的IVFuseNet中，红外图像和可见光图像都被使用，以平衡这两种图像的互补性。红外图像和可见光图像互为“辅助变量”。因为每一对红外线-可见光图像代表同一个场景。我们假设这两种图像存在共同特征。Wang等人[32]提出利用耦合过滤器可以将某些相似特征从源域转移到目标域。然而，即使经过高度复杂的操作，红外和可见光图像的所有特征也不是都能被表示和传递。在本文中，能够相互转移的特征称为“共同特征”，不能相互转移的特征称为“个体特征”。在上述分析的基础上，我们首先设计了共特征融合子网络进行共特征融合。\nP2 ：网络结构\n\n双流程结构：我们的共同特征融合子网络的详细机制如图2中的红色点框所示。我们采用的是双流程结构。\nAlexNet为每个流的基础：对于每个流，我们使用AlexNet[33]作为基础，因为AlexNet的参数数量相对较小。共特征融合子网络的具体内容如表1所示。此外，我们还可以选择其他网络作为基准，如VggNet[34]。该子网以256×512分辨率的红外图像及其对应的可见光图像作为子网的输入。\n部分耦合过滤器：与传统的双流CNN不同，我们在每个卷积层中设计部分耦合过滤器来学习红外和可见光图像之间的可转移特征。耦合比率如表2所示。\n\n\n\n\nP3：过滤器\n\n\n\n如图3所示，共特征融合子网络的过滤器可以分为三类:\n\n\n红外图像过滤器、\n\n\n可见光图像过滤器\n\n\n红外和可见光图像的部分耦合过滤器。\n\n\n\n\n部分耦合过滤器设计用于提取红外和可见光图像的特征。红外图像作为可见光图像的“辅助变量”，有助于发现弱光条件下可见光图像中未捕捉到的更有鉴别性的特征。同样，可见光图像作为红外图像的“辅助变量”，可以通过部分耦合过滤器从红外图像中提取出更多的细节信息(纹理、边缘等)。\n\n\n未耦和过滤器学习红外图像和可见光图像的单个特征。\n\n\n部分耦合过滤器数目与过滤器总数之比称为耦合比\n\n\n\n其中Ri为第i层的耦合比，ki为第i卷积层部分耦合过滤器的个数，ni为第i卷积层全部过滤器的个数。在本文中，我们使用以下耦合比率:0,0.25,0.5,0.75。我们在第4节中通过粗网格搜索演示了这些耦合比率的性能\nP4：耦合比\n\n可以注意到，耦合比随着卷积层的增加而增大。\n分析表明，浅层卷积层提取的纹理和细节特征在红外图像和可见光图像中有很大的不同。更深层次的卷积层提取红外图像和可见光图像的结构和形状特征，这些特征共享红外图像和可见光图像的共同信息。\n\nP5：网络训练方法\n在共特征融合网络中，利用BP算法更新滤波权值。可以发现，无论红外图像流还是可见光图像流，耦合过滤器的权值在每次训练迭代中更新两次，而非耦合过滤器的权值在每次迭代中更新一次。因此，假设我们先更新红外流的权值，再更新可见光流的权值，过滤器权值更新如下:\n\n\n其中n为迭代次数，μ为学习率，L为损失函数。耦合过滤器的权值在每次迭代更新如下:\n\nP6：特点和接下来的介绍\n综上所述，在共特征融合子网络中设计部分耦合过滤器，学习红外和可见光图像共同特征间的非线性变换关系，相当于增强两种图像的特征，融合共同特征。\n从红外图像和可见光图像中提取的特征分为共同特征和个体特征。为了同时利用公共特征和个体特征进行深度预测，我们提出了全特征融合子网络来融合双流结构的所有特征。全特征融合子网络的效率将在第3.2节中进行分析。\n全特征融合子网络\n自适应特征融合结构\n部分耦合过滤器提取公共特征后，需要将公共特征与个体特征进行融合进行深度预测。由于提取的特征具有不同的特性，因此设计一种自适应的特征融合策略至关重要。在本文中，我们在图2所示的全特征融合子网络中提出了一种自适应加权融合策略。\n该加权融合策略由三个步骤组成 : 设f_ir∈b×w×h×c和f_vi∈b×w×h×c是从共同特征融合子网络提取的特征\n\n首先:我们将fir和fvi在第三个维度上进行拼接，这相当于融合f_fusion∈b×w×h×2c的红外图像和可见光图像的特征。\n第二:将融合特征与核k∈2c×c×1×1进行卷积，其中2c为输入通道数，c为输出通道数，核大小为1×1。受[43]的启发，由于融合特征f_fusion同时考虑了红外和可见光图像的特征，因此该卷积运算的输出与这两种特征都有关。因此，这个程序学习了两种特征之间的相关性。在得到f_ir或f_vi的相同维数的初始系数矩阵M后，我们计算它们的点积，表示它们对不同场景深度预测的不同贡献。\n第三，我们设计了一个sigmoid层，将M中的每个元素转换成0到1之间的概率形式。经过这三个步骤，最终得到系数矩阵g。其过程如下:\n\n\n其中b为批大小,n代表输出通道的数量.我们令红外图像的系数矩阵G_ir = G,可见光图像的系数矩阵为G_vi =1 - G, 其中G_ir和G_vi分别表示红外图像和可见光图像对深度预测的贡献水平。\n在不同的条件下，系数矩阵是不同的.例如,G_ir可能在暗光情况下比G_vi大而,G_vi可能在白天下比G_ir大.G_ir和G_vi分别表示红外图像和可见光图像特征的贡献，具体如下:\n\n其中’圈点’代表点积.全融合特征可以这样得到:\n\n全特征融合子网络决定了红外特征和可见光特征对深度预测的依赖程度。\n高分辨率重建子网络\nP1：网络结构\n利用共特征融合子网络和全特征融合子网络，获得了分辨率为真值图1 / 16的融合特征。为了预测深度图，我们需要将低分辨率的深度图映射到与真值图具有相同分辨率的高分辨率深度图上。如果采用传统的反褶积方法进行深度预测，将会产生棋盘状伪影，并丢失详细信息。受[28,29]的启发，我们设计了一种基于残差密集卷积块的高分辨率重构子网络。图4为高分辨率重建子网图。\n\nP2：原理\n令f_fusion’为从全特征融合网络中得到的低分辨率特征.\n\n首先, 我们使用卷积层从红外和可见光图像的低分辨率融合特征中学习信息。\n\n\n其中g()为3×3卷积与批处理归一化(BN)的复合函数。\n\n然后将F0作为残差密集卷积块的输入。在残差密集卷积块中，各层的输出直接与当前残差密集卷积块的后续各层相连。它还允许将前一残差密集卷积块的输出连接到当前残差密集卷积块的各层.\n\n\n其中n为第n个残密卷积块，i为当前残密卷积块的第i层。如果每个剩余密度卷积的输出块N0特性和每一层的每一块有N个特性,我们连接的特性由前面的块和前层当前的块,导致𝑁0 +(𝑖−1)𝑁特征图的输入层。在这篇文章中,我们设置𝑁0 =𝑁= 32和每一块有三个复合操作的卷积,批量标准化和纠正线性单元\n与传统的卷积神经网络相比，残差密集卷积块可以利用原始低分辨率特征的层次特征增强细节特征。在得到拼接的特征图后，我们采用1×1卷积运算自适应地学习各层的特征，并使通道数与F0相同进行残差学习。通过3个残差密集卷积块，使层间信息最大，得到信息更详细的特征图。然后利用残差学习，加上F0和最后一个块的输出，进一步增强了网络的预测能力。最后，我们设计了一个上采样层而不是反卷积层，该层的输出大小为256×512，是我们预测的深度图。我们将在第4节演示高分辨率重建的有效性.\n实施细节\n\n\n深度学习框架:TensorFlow\n\n\n显卡:NVIDIA GTX 1080Ti, 11G显存\n\n\n初始参数:为了对不同方法进行比较，尽可能减少参数设置对最终深度预测的影响，本模型中各方法的初始参数设置相同。具体来说,权重都是初始化的截断正态分布的标准差σ= 2∕(𝑘×𝑘×𝑛_𝑖𝑛)\n\n\nk是内核大小,外祖母是渠道的数量输入,𝜇是期望和截断范围(a, b)\n\n\n所有方法的学习率初始为1e−4，每20000次迭代降低0.9倍，所有方法的总迭代次数为80,000次。经过80000次迭代，所有方法都可以收敛;批量大小为4，每个卷积层后使用批量归一化处理。为了防止训练过程中出现过拟合，我们将每个激活函数层后的dropout操作设置为0.8。对于训练损失，选择交叉熵作为我们的损失，利用Adam优化器对网络进行优化。\n\n\n其中L是损失，y_predictedi是实际预测，y_i是预期预测.\nExperiments and results\n在本节中，我们将介绍我们的NUST-SR数据集和实验的评价指标。通过对数据集的综合分析，我们证明了共特征融合子网络、全特征融合子网络和高分辨率重建子网络的有效性。与全CNN、多尺度CNN等9种方法相比，IVFuseNet在该数据集上的性能最好[9,10,19,30,33 - 37]。\n数据集和评估方式\nP1-P2：本文的数据集NUST-SR\n大多数基于视觉的无人驾驶深度预测研究仅关注白天可见光图像的情况。然而，无人驾驶车辆在低光照条件下行驶的情况也应予以考虑。既包含红外图像又包含可见光图像的开源数据集很少。\n本文使用的是NUST-SR数据集，该数据集由无人驾驶车辆在白天和黑夜行驶时拍摄的实际道路场景组成。车载远红外相机、车载RGB相机和Lidar采集的原始NUST-SR数据集，包含分辨率为768×576的红外和可见光图像以及深度雷达数据。在原始数据集中，并非每个点都有深度值，如图5(a)所示，我们首先在NYUDepth development kit[9]中使用Levin等人的着色方案填充不存在深度值的点，如图5(b)所示。\nP3：使用分类方法预测深度\n\n此外，我们使用分类方法来预测深度，而不是以往的回归方法[19,38,39]。原因是真实场景比较复杂，预测每个像素的深度值要比预测深度范围困难得多，而且我们对远、近物体深度预测精度的要求也不一样。这种分类方法使得我们对近场景[38]有较高的准确率，而对远场景[38]有较低的准确率。对于不同的层数，如图5所示，级数越大，真值越详细，越接近原始深度图。我们可以发现，16级的深度图相对于32或64级的深度图来说是比较粗糙的，所以我们重点比较了32级和64级的深度图。64级的深度图接近比32层的深度图更接近真值,但获得的参数的数量和失败的训练次数,63级别分别比32级高7%和22%,但预测的深度地图两种方法之间的差别很小。因此，本文将深度图在对数空间划分为32个级别作为训练标签，如图6©所示。最后，我们将红外图像、可见光图像和深度图裁剪到256×512的分辨率。通过对原始数据进行预处理，得到了NUST-SR数据集，该数据集拥有标注红外图像、可见光图像和对应的真值图,白天6529幅, 夜间5612幅。\nP4-P5：评价标准\n在NUST-SR数据集的测试图像上，我们使用四个指标对提出的IVFuseNet进行评估:\n\n方均根误差(RMSE)\n平均相对误差(Rel)\n平均对数误差\n识别准确率\n\n红外与可见光融合的结果\n\n\n\n密集卷积块的结果\n\n\n和相关方法的对比\n\n\nConclusion\n主要贡献\n\n我们提出了一种基于CNN的结构，称为IVFuseNet，它可以自适应融合红外和可见光图像的互补性，以解决不同光照条件下的深度预测问题。我们提出的融合方法主要包括两个方面。\n\n首先，我们在共特征融合子网络中设计部分耦合过滤器，利用“辅助变量”增强红外和可见光图像的特征。\n其次，为了考虑不同光照条件下红外和可见光图像的不同贡献，设计了自适应加权融合方法。\n此外，我们引入3个残差密集卷积块，进一步恢复细节，提高深度预测的准确性.\n\n\nIVFuseNet的有效性已经在我们的NUST-SR数据集上得到了验证。与其他方法相比，我们在该数据集上取得了最好的性能。\n\n未来工作\n\n收集新的数据集和设计新的深度预测结构。目前无人驾驶深度预测主要集中在白天应用，现有的数据集都是在良好的光照条件下设计的。在不同的天气条件和光照条件下，需要开发新的数据集进行进一步的研究。\n本文不考虑图像的序列信息。在未来，我们将尝试采用递归神经网络来处理图像序列中不同帧之间的相关信息。\n\n","categories":["Paper Reading"],"tags":["infrared light","visible light","information fusion","depth prediction"]},{"title":"Radar Camera Fusion via Representation Learning in Autonomous Driving | Pre-published","url":"/2021/03/24/RCF-Represent_learning/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\nAbstract\n\n背景：雷达和图像是成熟，低成本，鲁棒并且广泛应用在产品级的感知手段，由于他们的互补性，雷达检测（点云）和图像（2D检测框）通常被融合后生成最优感知结果。雷达-图像融合成功的关键点是数据关联。\n工作：我们想通过深度表征学习（deep representation learning）进行rad-cam关联，来进行特征级的交互和全局的推理。\n\n设计了一种损失采样机理和新型序数损失（ordinal loss ）来克服 标签不准确 和强化严格的人类推理（enforce critical human reasoning）\n\n\n结果：\n\n尽管在使用具有噪声的标签进行训练的情况下，我们提出的方法实现了92.2% 的F1分数，这比基于规则的老师算法提升了11.6%。\n除此之外，这种基于数据的方法同样使得其通过极端情况（corner case）挖掘实现不断的性能提升。\n\n\n\nIntroduction\nP1-2: Lidar的缺点&amp;radar的优点\nLiDAR的缺点：\n\n容易出现错误\n购买，维护成本高\n商品级雷达尚未满足需求\n\n毫米波radar的优点：\n\n距离速度估计相对精确\n鲁棒性高，低成本，维护成本低\n\nP3:传统radar融合\n传统的radar-carmera fusion：\n\n\n基于关联规则的算法和基于运动学模型的跟踪，关键点是将雷达观测和相机观测进行配准\n\n\n缺点：传统的关联过程是基于最小化特定的距离度量和一些启发式规则“手动”操作的。它不仅需要大量的工程和调优，而且很难适应不断增长的数据。\n\n\n[16,25,28]: 结合雷达和相机数据作为输入进行3D目标检测，这些方法都以激光雷达数据作为真值来建立radar和相机的关系。对于大部分公开数据集例如nuScenes，Waymo这是足够的，但是 他们不能被部署到大部分商用车辆上。\n在这项研究中，我们提出了一个基于可扩展学习的框架来关联雷达和相机信息。\nP4：我们的方法\n我们的目标：找到雷达和相机检测结果的一个表征形式，能使得配对后的距离近而未配对的距离远。我们将检测结果转换到图像通道上然后叠加到原图上，再进入名为AssociationNet的CNN网络中。训练是通过从基于传统的基于规则的关联方法（rule-based association method）获得的不完美的标签上进行的。通过引入损失采样机制可以减少错误标签。为了进一步提升性能，我们通过增加了一个新型的序列损失（ordinal loss）。该网络通过对于实际场景的推理，显著地超越了基于规则的方法。\nP5：我们的主要贡献\n\n我们提出了一种可扩展的、基于学习的雷达-相机融合框架，无需使用激光雷达的地面真实标签，适合在真实的自动驾驶应用中构建低成本、可生产的感知系统。\n我们设计了一个损耗采样机制来减轻标签噪声的影响，并发明了一个顺序损耗来加强关键关联逻辑到模型中以提高性能。\n我们通过表征学习发展了一种鲁棒性的模型，可以操作多种具有挑战性的场景，，同时它的F1分数比传统的基于规则的算法提升了约11.6%\n\nRelated Work\n2.1 Sensor Fusion\n\n\n[9,17,19,12,31]: 目标级融合（object-level fusion）,各个传感器分别处理数据检测目标，然后融合算法融合目标检测结果形成全局机动跟踪航迹。\n\n\n[1,5]: 3D目标检测和多目标跟踪\n传统的方法倾向于手动设计各种距离指标，来表示不同传感器输出之间的相似性。\n\n\n[2]距离最小化和其他启发式规则被用来找到关联方式。为了处理复杂性和不确定性，在关联过程中有时也采用概率模型\n\n\n2.2 Learning-Based Radar-Camera Fusion\n\n[28,14]: 前期融合\n[16,7,26]:中期融合\n\n由于从检测结果得到的信息太少，基于融合的目标级融合仍待探索。在本研究中，我们提出的方法属于这一类：我们聚焦于关联雷达和相机的检测结果。因此，我们的方法和传统的传感器融合流程更加兼容。另外一方面，我们的方法直接采用原始相机和雷达数据来进行性能提升增强。\n2.3 CNN for Heterogeneous Data\nCNN在结构化图片上的巨大成功激发了人们将其应用在其他多种类型的数据上，例如传感器参数、点云和两类数据的关联关系上。为了能和CNN进行兼容，一种流行的方法是将异类数据转化为“伪图片”的形式。例子包括\n\n\n[11]: 用归一化坐标和视场映射将摄像机内在编码到图像中(? ? ?)\n\n\n[6,28]: 将雷达数据投影到图像平面，形成新的图像通道\n\n\n[30,23]:各种形式的基于投影的激光雷达点云表示\n\n\n我们采用了相似的方法处理雷达和相机输出。\n2.4 Representation Learning\n表征学习被认为是理解复杂环境和问题的关键[3,20,18]。表征学习被广泛应用于许多自然语言处理任务，如单词嵌入[24]，以及许多计算机视觉任务，如图像分类[8]，目标检测[13]，关键点匹配[10]。在本研究中，我们的目标是在高维特征空间中学习一个向量作为场景中每个对象的表征，以建立对象之间的交互，并进行场景的全局推理。\nProblem Formulation\n\n\n传感器：\n\n相机：FOV:120度，52度；像素：1828*948；频率：10Hz\n\n\n\n雷达：输出的是一系列经过后处理的点，这些点据有一些属性，也叫做雷达针（radar pins）。这些雷达针是在目标级上的。每帧输出都可能有几十个雷达针。\n\n\n\n\n\n\n\n雷达针值得注意的点：\n\n在BEV视角下进行2D信息处理。不考虑仰角，因为在纵向上精度很差\n每一个雷达针都对应了一个移动的目标（例如车、自行车、行人等）或者是某种具有干扰的静态结构（例如交通牌、路灯、桥等）\n\n\n\n在这项研究中，我们聚焦于将来自相机的2D识别框和雷达针相关联。在具有精确关联情况下，许多类似3D目标检测和跟踪的任务会变得更加简单。\nMethods\n我们的工作主要包括：\n\n\n前处理：配准雷达和相机数据\n\n\n基于CNN的表征学习网络 AssociationNet\n\n\n后处理：表征提取和关联\n\n\n\n4.1 Radar and Camera Data Preprocessing\n前处理：进行时-空配准。(temporal and spatial alignment）\n\n时间配准：将和图片帧时间最近的雷达帧相互进行配准。\n空间配准：利用已知信息将雷达针从雷达坐标进一步转换到相机坐标。雷达针的所有属性将在AssociationNet中使用。\n\n相机的每帧图像都会先进行2D目标检测并输出检测框，检测框的属性见表2。虽然在本研究中，网络使用的是RetinaNet，其实任何2D检测网络都可。经过前处理，一系列时–空配准的雷达针和相机检测框就可以进行接下来的关联了。\n\n4.2 Deep Association by Representation Learning\nP1: 表征学习的原理\n利用AssociationNet学习每个雷达针和每个检测框的语义表征信息。在这个表征下，一对匹配的雷达针和检测框将会”看上去“相似，即他们的学习表征距离比较接近。 这个过程的大致描述见图2.\nP2：Process a：得到伪图像；Process b：叠加原图\n为了利用强大的CNN架构工具，我们将每个雷达针和2D检测框投影到像平面来产生一个伪图像（pseudo-image），每一种属性都占据了一个独立的通道。\nProcess a：\n\n每一个检测框被分配到其中心的像素位置上\n每一个雷达针通过将其3D位置投影到相平面上来分类到像素位置上\n\n接下来，我们将原始RGB相机图像和相应的伪图像叠加以合并丰富的像素级信息。然后应用AssociationNet进行表征学习。\nP3: AssociationNet的结构；process c：提取表征向量\n\n\nBackbone：ResNet\nFPN：融合不同尺度的特征\ntwo extra layers ：将特征恢复到输入图的尺寸\n\n输出的特征图包括了雷达针和检测框的高维语义表征信息。每个雷达针和检测框都在特征图中有独有的像素位置，我们在输出的特征图的对应像素位置提取他们的表征向量。这个过程就是process c。\nP4: 网络的输入及输出\n\n\n输入-伪图：\n\n7个雷达针通道：object-id, obstacle-prob, position-x, position-y, velocity-x,\nvelocity-y, heatmap\n4个检测框通道：height, width, category, heatmap\n3个原图RGB通道\n\n\n\n输出-特征图：\n\n总共128通道\n每个雷达针的表征向量 64维\n每个检测框的表征向量 64维\n\n\n\nP5：损失函数\n我们所得到的表征向量代表了雷达针和检测框在高维空间中的语义含义。如果一个雷达对表示了现实世界中的同一个目标，我们就把它作为一个正样本，反之为负样本。我们尝试最小化所有的正样本的表征向量之间的距离，并且将负样本的表征向量之间的距离最大化。基于这个逻辑，我们设计了根据关联真值标签的损失函数。我们将这些正样本的表征向量加和到一起，得到吸引损失（pull loss）：\n\n我们将负样本的损失加和，得到排斥损失（push loss）：\n\n\n\nPOS,NEG:正样本集、负样本集\n\n\nnpos，nneg：POS中的关联数和NEG中的关联数\n\n\n（i1,i2）:第i个关联对，包含雷达针i1，检测框i2\n\n\nhi1，hi2：代表对应的学习出的表征向量\n\n\nm1，m2：设想的表征向量间的距离的阈值，我们设定为了2.0和8.0\n在推理过程中，我们计算了所有可能的雷达针-检测框对的表示向量之间的欧氏距离。如果距离低于一定阈值，则认为雷达针与检测框成功关联。\n\n\n4.2.1 Loss Sampling\n用于监督学习过程的关联标签基本上来自于传统的基于规则的方法，这些标签远远没有达到100%精确度并且包含了部分噪声。\n\n训练时：过滤某些关联标签：为了减轻不准确的标签的影响，我们首先通过过滤掉了某些低可信度的关联对，以纯化标签。这提升了剩余关联标签的精确度，但是代价是破坏了回归结果（undermined recall）。\n训练时：过滤某些排斥标签对在训练AssociationNet 时，排斥损失的计算过程中，我们只取样了一部分用来训练，以减少错误地分开了正关联对。取样的负关联对的数目和正关联对的数目相等。\n\n4.2.2 Ordinal Loss\nAssociationNet 犯的一个特殊类型的错误是：它可能会违反简单顺序规则(simple ordinal rule),即这种现象：一个较远的雷达针关联到一个较近的检测框，一个较近的雷达针关联到了较远的检测框。为了解决这个情况，我们提出了顺序损失。\n将检测框i的底部的y坐标设为ymax_i，相应的3D世界的深度为d_i。对于任意两个在同一张图片上的检测框，我们有如下的性质：\n\n物体在3D世界中的顺序可以通过检测框底部的相对距离次序来推测。因此，我们设计了一个额外的顺序损失根据顺序规则来强化自身一致性（self-consistency ），这个损失是：\n\n\n^POS:预测的正关联集\n^n_pos:^POS的大小\ni1，i2：第i个预测关联的雷达针和检测框\nj1，j2：第j个预测关联的雷达针和检测框\nd*：相机坐标系下雷达针的深度\ny*_max：检测框底部的y坐标\nσ：激活函数，用来平滑损失值\n\n最后, 总损失为:\n\n其中w_ord是用来平衡损失的权重系数.\n4.3 Training and Inference\n\n训练:\n\n硬件:2080Ti\nbatch size: 48\n训练方法:SGD,10K次迭代\n学习率:初始10^-4, 8K次迭代结束时下降10倍,9K次迭代结束时再下降10倍\n\n\n推理:\n\n首先, 使用训练后的模型先生成雷达针和检测框的表征向量\n然后计算一个关系矩阵(affinity matrix)，其中每个矩阵元素对应于雷达引脚的表示和一个包围框之间的距离。在实际过程中,一个检测框可能和多个雷达针关联,但是每个雷达针只能和最近的检测框关联。因此，我们设计每个雷达针与关系矩阵中距离最小的检测框相关联。\n最后,距离大于阈值的不可能的关联被过滤掉，这通常包括来自干扰静态物体的雷达针。\n整个推理过程见图4\n\n\n\n\n4.4 Evaluation\n在测试数据集中，将预测的关联与人为标注的关联进行比较。我们使用精度、回忆和F1分数作为评估性能的指标。\n在一些非常复杂的场景中，即使是人类的注释员，正确地将所有雷达针和检测框联系起来是非常具有挑战性的。因此，在评估过程中，我们将这些看似可信但不确定的关联标记为“不确定”。示例如图5所示。对于那些“不确定”的关联，它们既不被视为积极的联想，也不被视为消极的联想，这将被排除在正确和错误的正面预测之外。\n\nExperiments and Discussion\n5.1 Dataset\n​\tAssociationNet在一个内部数据集上进行了训练和评估，该数据集包括由测试车队收集的12个驾驶序列，包括在各种驾驶场景下14.8小时的驾驶，包括高速公路、城市和城市道路。雷达和摄像机最初以10Hz同步，然后进一步降采样到2Hz，以减少相邻帧之间的时间相关性。12个序列中的11个用于训练，另一个用于测试。因此，训练数据集中有同步雷达和摄像机帧数104,314个，测试数据集中有2,714个。对于训练数据，关联标签由传统的基于规则的算法生成，并附加过滤以提高精度。对于测试数据，我们用人工标注来手动整理标签，以获得高质量的ground-truth。\n5.2 Effect of Loss Sampling\n​\t采样比定义为每帧的正对数与负对数之比。结果如表3所示。我们可以看到，采用损耗采样机制的最佳采样比为1:1，在F1评分方面性能提高了1.1%。\n\n5.3. Effect of Ordinal Loss\n​\t顺序损失的影响如表4所示。顺序损失在一定程度上提高了AP和AR。在权重最优的情况下，F1成绩提高了1.8%。\n\n5.4 Comparison with Rule-Based Algorithm\n​\t我们比较了AssociationNet与传统基于规则的算法的性能，如表5所示。值得注意的是，虽然使用传统的基于规则的算法来生成关联标签来监督AssociationNet的训练，但AssociationNet的性能明显优于基于规则的替代方法。这说明了基于学习的算法在处理中固有的鲁棒性\n","categories":["Paper Reading"],"tags":["sensor fusion","radar","representation learning"]},{"title":"Real-Time Hybrid Multi-Sensor Fusion Framework for Perception in Autonomous Vehicles","url":"/2020/11/05/Real-Time-Hybrid-Multi-Sensor-Fusion-Framework-for-Perception-in-Autonomous-Vehicles/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\nAbstract\n目前有许多文献提出了利用各种不同传感器和融合方法的融合框架。目前，提升精度表现吸引了更多的注意；然而，在实际车辆上的实施的可行性很少受到关注。一些融合框架在实验室里用高性能平台可以得到很好的结果；但是在实际应用中，由于他们昂贵的加个和大量的计算需求，不能被应用在嵌入式计算机上。我们提出了一种新型的混合多传感器融合通道配置，为自动驾驶车辆执行环境感知，如道路分割、障碍检测和跟踪。这种融合框架使用所提出的**基于编码-解码的全卷积神经网络(encoder-decoder based Fully Convolutional Neural Network, FCNx)和传统的扩展卡尔曼滤波器(Extended Kalman Filter, EKF)**非线性状态估计方法。该混合框架的目标是提供一个成本效益高、轻量级、模块化和强鲁棒性(在传感器故障情况下)的融合系统解决方案。它使用了FCNx算法，相比于基准模型提高了道路检测的准确性，同时保持了实时效率，可用于自主车辆嵌入式计算机。在超过3000个道路场景上的测试表明，我们的融合算法相比于基本网络算法显示出了更好的性能表现。除此之外，该算法被部署到了实际车辆上并且利用车辆收集到的数据进行测试，进行了实时的环境感知。\nwhy？：现有算法只关注了精度提升，没有关注可实施性\nhow？：FCNx + EKF\nresult?：1.比现有算法的更好性能表现  2.被部署到了实际车辆上进行了实时感知\nIntroduction\nP1：自动驾驶车辆的工作流程\n自动驾驶车辆(autonomous vehicles, AV)需要解决的两个主要问题：\n\n自我定位——我自己在哪？：GNSS,IMU,vehicle odometry（里程记）\n环境感知——我旁边有些什么？:LiDAR, camera，radar\n\n定位和感知的结果被送到AV的路径规划算法中来决定下一步的动作。这些动作进一步送到电机控制器上来控制车辆运动。图1显示了这个工作流程：\n\n图1 自动驾驶车辆结构。该论文关注于感知\nP2：问题引入\n当前环境感知的研究主要包括传感器信号处理和各种融合算法\n\n\n传感器类别：相机，激光雷达，雷达\n融合算法类别：\n\n状态估计融合算法(i.e. KF)\n基于机器学习的算法(i.e. deep neural network, DNN)\n\n\n\n\n当前的研究还是侧重于精度的提升而没有考虑部署的可能性，所以需要一种高效，轻量，模块化和鲁棒的流程。因此我们需要能一种平衡融合融合模型的复杂度和真实世界实时应用性，且同时能提升环境感知精度的算法。\nP3：本文结构\n\n第二节：回顾传感器融合文献\n第三节：回顾和我们用的传感器相关的感知技术\n第四节：展示我们所利用的传感器融合流程概览，并且展示如何处理传感器数据和实施融合方法（即我们的FCNx的细节和传统EKF的回顾）\n第五节：实验流程，工具和数据集\n第六节：实验结果和评估\n第七节：结论\n\nFusion Systems Literature Review\n融合系统文献回顾\n\n\n[1-5]:关注于相机和激光雷达的融合，利用深度学习方法来进行各种计算机视觉的任务\n\n\n[6]:关注于单传感器(相机或激光雷达)和单一方法（深度学习或者传统计算机视觉）的技术\n\n\n[1,2,7]：关注于激光雷达+相机和一种融合方法\n\n\n[8-10]：关注于激光雷达+雷达+相机和状态估计方法(卡尔曼滤波或例子滤波)\n\n\n[11]：不考虑实际世界部署或者传感器的特殊情况的融合方法，回顾了自动驾驶车辆系统的技术\n\n\n[12]：回顾了环境感知算法和介绍了智能车辆，包括车道和道路检测、交通标志识别、车辆跟踪、行为分析和场景理解的建模方法。\n\n\n[13]：回顾了传感器和传感器融合方法\n\n\n[6]：使用两种分离的CNN网络提出了一个RGBD目标检测架构\n\n\n[1,2]：利用快速决策树分类器(boosted decision tree\nclassifier)融合激光雷达和相机数据，进行道路检测\n\n\n[3]：利用几何变换，提出了一种基于视觉的道路检测方法\n\n\n[4]：提出了一种统一的方法来进行联合分类、检测和语义分割，重点关注效率。\n\n\n[14]：仅仅利用激光雷达，利用FCN来进行基于深度学习的路面检测\n\n\n[15]：提出了一种高维传感器融合架构并且在ADAS系统上进行了测试，例如紧急刹车辅助。\n\n\n[16]：融合相机和激光雷达点云数据，利用几种FCNs和不同的融合策略：早期、中期和后期融合来进行路面检测\n\n\n[17]：利用毫米波雷达和单色相机(mono camera)融合来进行车辆检测和跟踪\n\n\n[18]：展示了一种叫做&quot;SegNet&quot;的解码-编码FCN进行道路和室内图像的像素语义分割\n\n\n[7]：提出了一种用于传感器融合的深度学习框架ChiNet，该框架利用立体摄像机在不同的分支上融合深度和外观特征。ChiNet有两个输入分支用于深度和外观，两个输出分支用于道路和对象的分割\n\n\n[8]：提出了一种基于二维激光扫描仪和摄像机传感器的融合系统，结合**无迹卡尔曼滤波(UKF)**和联合概率数据关联来检测车辆\n\n\n[9]：利用UKF来融合传感器数据来研究室内的轮椅位置估计问题\n\n\n[5]：提出了一种基于深度学习的基于激光雷达和摄像机的端到端城市驾驶融合框架\n\n\n[19]：采用两阶段融合的方式，利用立体相机和激光雷达提升车辆检测的性能\n\n\n[20]：争对复杂城市场景，利用CNN和KF，融合相机，2d激光雷达和先验地理道路地图，提出了一种车辆检测和跟踪系统\n\n\n[21]：提出了一个用于行人、自行车和车辆检测的摄像头系统，然后使用雷达和激光雷达改进跟踪和运动分类。\n\n\n[22]：提出了一种基于热敏摄像机和RGB摄像机深度学习的多光谱行人检测系统\n\n\n[23]：提出了一种卷积神经网络进行语义分割——全卷积神经网络(fully convolutional network, FCN)\n\n\n[24]：改进了[23]的结构，提出了反卷积网络(DeconvNet)，使用带有反卷积滤波器的解码层对下采样的特征图进行上采样。\n\n\n[25]：利用编码器层和解码器层之间的连接改进了DeconvNet。在U-Net中，另一种基于编码器-解码器的结构用于语义分割，所有的特征映射都通过剩余连接从编码层转移到解码层。\n\n\n[26]：Mask-RCNN\n\n\n[27]：Fast - RCNN\n我们的目标：提供一种低成本，轻量化的融合系统，它可以被部署到相对便宜和高效的嵌入式边缘计算电脑上\n\n\nAutonomous Vehicle Perception Sensors\n本文使用的传感器\n本文使用三种传感器，他们的位置分布如下\n\n\n\n相机：受到环境变化例如：遮挡、光照和天气变化\n\n\nLiDRA：受到雨、雪恶劣天气影响\n\n\n雷达：低探测率和低分辨率\n\n\n\n使用三种传感器并采用合适的传感器融合算法有助于确定各种传感器的失效情况。同时提高了整体的误差容限和精度。\n\n传感器的位置和探测范围\n相机\nP1：相机的工作原理\n\n\n相机是进行高分辨率的任务：类似目标分类、语义分割、场景感知和需要色彩感知的任务类似交通灯或信号确认的重要传感器。\n\n\n镜头接受光→感光元件(CCD or CMOS)→ADC（AD转换器）→ISP（image signal processor）→RGB pattern\n\n\n物体位置[x,y,z] → （透镜）→ 像平面[x’, y’]\n\n\n\n相机工作流程\nP2：相机的校准，两种感光元件\n\n\n校准相机：由于镜头扭曲了生成的图像，需要先对每个相机进行校准。\n\n\n校准方法：我们通过使用已知物体的图片(如棋盘)来矫正这些失真。棋盘的尺寸和几何形状是已知的;因此，对于那些扭曲，我们就可以计算通过相机输出图像与原始图像的比较来标定参数。\n\n\n两种感光元件\n\n\n\nCCD（Charge-Coupled Device） : 高光敏感度，更好的成像质量，耗电更好，发热大，价格贵\nCMOS（Complementary Metal Oxide Semiconductor）：低成本，低耗电量，数据处理更快——我们采用\n\n雷达\nP1：雷达的工作原理\n\n\n测量原理：雷达可以测量障碍物的相对位置信息（基于电磁波的传播时间）和相对速度（基于Doppler效应）\n\n\n环境天气影响：电磁波不会受到不同光线和天气条件的影响\n\n\n安装位置：汽车雷达通常安装在保险杠后或在车辆格栅;车辆的设计是为了确保雷达性能(雷达天线聚焦电磁能量的能力)不受影响，无论它们安装在哪里。\n\n\nFMCW：对于自动驾驶车辆，通常使用调频连续波雷达(Frequency-Modulated\nContinuous Wave，FMCW)[29]。FMCW雷达发射连续波或线性调频信号，其中频率随时间增加(或减少)，通常呈锯齿形\n\n\nMMW: 这些雷达通常工作在24,77,79GHz频段，对应于毫米级波长，所以也称之为毫米波雷达(millimeter wave) radar。根据应用可以分为三种\n\nSRR(Short-Range Radar)：辅助泊车，碰撞警告\nMMR(Medium-Range Radar)：盲点检测，侧/后方防撞\nLRR(Long-Range Radar)：自适应巡航，前方早期碰撞检测\n\n\n\n\nFMCW雷达工作流程\nP2：FMCW雷达工作流程\n有关雷达工作的主要流程,主要理解下列元件的作用即可，这部分略\n\nFrequency Synthesizer\npower amplifier\ntransmit (TX) antenna\nreceive (RX) antenna\nLNA(Low Noise Amplifier)\nMixer\nADC(analog digital converter  )\n\n激光雷达\nP1：激光雷达的工作原理\n\n\n工作原理：发射激光或红外(IR)光束，接收光束的反射来测量周围环境。用于自动驾驶汽车的激光雷达大多波长在900纳米(nm)，但有些波长更长，在雾或雨中工作得更好。\n\n\n输出：激光雷达输出点云数据(point cloud data, PCD), 其包含物体位置(x,y,z)和强度信息。强度值表示物体的反射率(物体反射的光量)。利用光速和飞行时间(time of flight, ToF)来检测物体的位置。\n\n\n两种类型\n\n\n机械/转子激光雷达：使用电机旋转来发射和接受激光/红外线\n固体态激光雷达(Solid State LiDAR, SSL)：没有任何旋转镜头来引导激光;相反，他们用电子方式控制激光。这些激光雷达比机械激光雷达更坚固、可靠、更便宜，但其缺点是与机械激光雷达相比，它们的体积更小，FoV更有限。\n\n\n\n\n人体危害：LiDAR输出的是最弱的1类激光，对人体无害\n\n\n\n激光雷达的工作流程\nP2：激光雷达的工作流程\n\n激光由二极管产生\n通过旋转镜头将能量聚焦到各个角度上\n光被反射后背另外一套旋转镜头接受\n计时器计算ToF来确定目标位置信息\n产生的点云信息传递给其他设备\n\nP3：激光雷达的特点\n\n\n和相机相比：\n\n相同点，LiDAR产生2D空间信息\n不同点：不同的是，它通过获得距离来创建3D空间环境信息。通过测量距离，LiDAR提升了角分辨率(水平和垂直方向)相比于相机获得了更好的量测精度。\n\n\n\n和雷达相比：\n\n激光雷达具有更高频率和更短的波长，更精确\n\n\n\n典型的激光雷达精度：\n\n\n测量距离：40-100m\n\n\n分辨率精度1.5-10cm\n\n\n垂直角分辨率：0.35-2度\n\n\n水平角分辨率：0.2度\n\n\n采样频率：10-20Hz\n\n\n\n\nHybrid Sensor Fusion Algorithm Overview\n混合传感器融合算法概述\n算法分为两个部分，他们彼此并行，如图6。在每个子部分中，选用了最适合当前任务的一系列传感器参数和融合方法。\n算法第一部分：LiDAR + camera\n\n\n处理高分辨率任务：如目标分类，定位和道路语义分割\n\n\n使用LiDAR + camera\n\n\n卷积神经网络CNN和全卷积网络FCN相比于传统方法，即特征提取器(SIFT, HOG)具有更好的性能表现\n\n\nFCN结构的融合是最好的\n\n\n我们优先考虑实时性能和精度，所以我们没有像[32]中那样将道路场景分割为多个场景进行分割，我们只分割了两类：可驾驶区域和不可驾驶区域\n\n\n我们还利用了高效和快速的网络例如YOLO V3[33]来探测分割后路面的障碍物。\n\n\n算法第二部分：LiDAR + radar\n\n处理目标检测和状态跟踪任务\n使用LiDAR + radar\n两种传感器的数据在目标层(object level)进行融合\n激光雷达和雷达数据处理会在感兴趣区域(ROI)内产生障碍物簇及其状态。处理后的传感器数据在目标层上的融合称为后期融合。结果融合后的激光雷达和雷达数据被发送到一种状态估计方法，以最佳组合每个传感器的噪声测量状态。\n状态估计方法采用KF，但是是争对线性模型的\n因为像汽车这样的障碍物的运动是非线性的;我们使用改进版的KF，即扩展卡尔曼滤波器(EKF)。了解和跟踪道路上障碍物的状态有助于预测和解释它们在AV路径规划和决策堆栈中的行为。最后，我们覆盖每个融合输出并在汽车监视器上显示它们\n\n利用深度学习进行目标分类和道路分割\n\n\nraw data combine\n\n\nRGBD图像，其中D为深度通道\n\n\n将这些数据传输给我们提出的全卷积层神经网络FCNx\n\n\n**FCNx is an encoder-decoder-based network with modified filter numbers and sizes in the downsampling and upsampling portions and improved skip connections to the decoder section.  **\n\n\nFCNx是一种基于编码-解码的网络，它拥有一些在下采样和上采样过程中，数量和大小参数都修正过的滤波器；还在解码环节拥有改进跳跃连接。\n\n\n相机-激光雷达Raw Data融合\n\n数据准备：\n\n相机：2D图像，RGB\n激光雷达：除了点云图像，还有高分辨率深度图\n\n\n早期融合：\n\n我们将raw data结合在了一起\n得到RGBD图像\n\n\n融合过程：\n\nto FCN：进行道路语义分割\nto CNN：目标检测和定位\n\n\n\n\n相机-激光雷达融合流程图\n全卷积神经网络结构FCNx\n结构包括两个主要部分：编码器和解码器\n\n编码器：\n\n基于VGG16[34]\nVGG16的全连接层替换为卷积层\n编码器的目的是从RGBD图像提取特征和空间信息\n它使用一系列的卷积层来提取特征，池化层来减小特征图的尺寸\n\n\n解码器：\n\n基于FCN8[23]\n其目的是在保持低、高层信息完整性的前提下，将像素类的预测重新恢复到原始图像大小。\n它采用转置的卷积层来对编码器的最后一个卷积层的输出进行采样\n它还采用跳跃卷积层来结合来自编码器的精细的低等级特征(finer low-level features)和转置(上采样)卷积层的粗糙的高等级特征(coarser high-level features)。\n\n\n\n\n网络结构\n网络结构介绍：\n\n\n将第四层的编码层的输出和第七层上采样层结合。这个结合是元素意义上的合并。这个结合后的特征图被添加到第三层的跳跃连接上。\n\n\n…\n原文直接抛出来把，这里太乱了…给我看晕了都\n\nIn our proposed network, which builds on the above shown in Figure 8, we combine the encoder output from layer four with the upsampled layer seven. This combination is an element-wise addition. This combined feature map is then added to the output of the third layer skip connection. In the skip connection of the layer three output, we use a second convolutional layer to further extract features from layer three output. The addition of this convolutional layer adds some features that would be extracted in layer four. Including some basic layer four level feature maps will help the layer three skip connection to represent a combined feature map of layer three and layer four. This combined feature map is then added to the upsampled layer nine, which itself represents a combined feature map of layer seven and layer four. This addition is shown to give a better accuracy and lower Cross-Entropy loss compared to the base VGG16-FCN8 architecture. The better performance can be explained by the fact that having some similar layer four feature maps can help better align the extracted features when performing the last addition. We will refer to our proposed architecture as FCNx\n\n\n\n优点：相比于基础的VGG16-FCN8网络，拥有更高的精度和更低的交叉熵损失（lower Cross-Entropy loss）\n\n\n原因：当进行最后的添加操作时，拥有比较相似的四个特征图可以帮助更好地对齐提取的特征。\n\n\n利用kalman滤波进行障碍物检测和跟踪实验\n在本节中，我们利用雷达和激光雷达数据来检测障碍物和测量他们的状态。我们进行了后数据融合或者说是目标级的融合。然后，利用非线性卡尔曼滤波方法，利用这些噪声传感器来估计和跟踪障碍物状态，其精度比单个传感器都要高。\n雷达拍频信号（beat signal）处理\n雷达传感器可以探测到障碍物和他们的状态，通过图9的过程：\n\n\n\n第一步：带噪声信号 → (雷达内部ADC) → 数字信号\n\n处理从雷达接收到的有噪声的数字化混合信号或拍频信号。拍频信号通过雷达内部的ADC发送，并转换为数字信号。数字信号在时域内，由多频分量组成。\n\n\n\n第二步：时域信号 → (1D-FFT) → 频域信号\n\n\n1D - FFT也被称为1st stage or Range FFT。分离它的全部频率分量。FFT的结果是被单位为dBm代表的信号强度或者分值，横坐标是拍频的频率MHz。每个频率峰代表一个检测到的目标。\n\n\nx轴可以从目标的拍频的频率转换为目标的距离，通过公式：\n\n\n\n\n$$\nR=\\frac{c \\cdot T_{\\text {sweep}} \\cdot f_{b}}{2 \\cdot B_{\\text {sweep}}}\n$$\n\n\n其中R：目标距离；c：光速；T_sweep &amp; B_sweep：啁啾/扫描时间和带宽\n\n\nT_sweep &amp; B_sweep的计算公式：\n\n\n$$\nT_{\\text {sweep}}=5.5 \\cdot \\frac{2 \\cdot R_{\\max }}{c}\n$$\n$$\nB_{\\text {swee } p}=\\frac{c}{2 \\cdot d_{\\text {res}}}\n$$\n\nR_max是最大距离，d_res是雷达距离分辨率。常数5.5来自假设：对应于最大雷达扫描距离，扫描时间通常是延迟的5-6倍\n\n如上所讨论的，range FFT可以告诉我们拍频，峰值，相位和距离。为了测量目标的速度（Doppler velocity），我们需要找到多普勒频移，即雷达chirps的相位变化率。目标从一个chirps变化到另外一个。因此，在得到range FFT后，进行第三步。\n\n\n第三步：Doppler FFT 测量相位的变化率——多普勒频移\n\n\n输出结果是一个3D图，分别由信号强度，距离和多普勒速度代表\n\n\n这个3D图也叫做**(Range Doppler Map, RDM)**。RDM给出了我们目标的距离和速度的概览\n\n\n\n\n\n第四步：过滤RDM的噪声和杂波\n\n\n最常用方法：Cell Averaging CFAR (CA-CFAR) [38]\n\nCFAR : 它会根据信号局部噪声的大小动态调整阈值，如图10，CA-CFAR在我们生成的2D FFT图像上生成一个滑动窗口。窗口包括：\n\nCell Under Test (CUT)：正在被测试目标是否出现的格子\nguard cells (GC)：CUT附近的格子，防止目标信号泄漏到周围的单元，从而对噪声估计产生负面影响\nreference or training cells (RC) ：包住CUT周围环境的格子\n噪声的等级是根据RC的平均值来估计的，如果CUT中的信号比该阈值低，则它被去除；如果比阈值高，那么就认为检测到了目标。\n\n\n\n\n\n\n最后，我们实现了具有距离和多普勒速度信息的雷达CFAR检测，但目标的实时检测和跟踪是一个计算量大的过程。因此，我们将属于同一障碍物的雷达检测聚类在一起，以提升该流程的性能。我们使用基于欧氏距离的聚类算法。该方法将目标大小范围内的所有雷达探测点视为一个聚类。为该聚类分配一个中心距离和速度，该距离和速度等于所有聚类检测距离和速度的平均值\n激光雷达点云数据处理\n\n\n第一步：数据下采样和滤波\n\n激光雷达点云数据数据量巨大，为了适应算力和实时性要求，需要适当降低数据量\n利用立体像素网格(voxel grid)，我们在（point cloud data，PCD)中定义了一个立方体素，并且每个体素只分配一个点云。\n下采样之后，我们利用ROI（region of interest）来将超出这个范围的PCD给过滤掉（就是过滤掉不是路面的物体例如楼房）。\n在后续的步骤中，我们将过滤后的PCD分割为道路和障碍物\n\n\n\n第二步：PCD分割\n\nPCD分割在点级别上进行，十分耗费计算资源，所以我们需要进行处理\n类似于雷达数据，我们将障碍物和它们附近的点（欧式距离）集群，并且给每个群一个新的位置坐标（x,y,z），这个坐标是群内所有点的坐标的平均值。最后，我们定义群大小的bounding box的尺寸，并且利用这个box来将障碍物进一步可视化。\n\n\n为了将PCD分割为道路和障碍物，我们需要找到道路平面，我们利用随机取样共识Random Sample Consensus(RANSAC)[39]。在每一步，RANSAC都会从我们的点云中选取一个样本，然后用一个平面穿过它。我们认为道路点是inliers和障碍点是outliers。它测量inliers(属于道路平面的点)并返回该平面，即具有最高inliers数量或最低离群值(障碍)数量的道路平面。\n\n\nRSNSAC实在有点不好理解，已附上原文：\n\nAt each step, RANSAC picks a sample of our point clouds and fits a plane through it. We consider the road points as inliers and obstacle points as outliers. It measures the inliers (points belonging to the road plane) and returns the plane i.e., road plane with the highest number of inliers or the lowest number of outliers (obstacles)\n\n拓展kalman滤波\n在利用FCNx进行道路语义分割后，本节我们利用EKF实施单目标跟踪。\n\n\n一个标准的KF包括三个步骤：初始话，预测和更新。\n\n\n传统的KF处理不了非线性模型，非高斯噪声，我们用了EKF\n\n\nLiDAR：得到位置Px，Py\n\n\nradar：得到位置Px，Py以及速度Vx，Vy ；但是是在极坐标系下。\n\n\n我们需要将笛卡尔坐标系转换为极坐标系，得到非线性的H\n\n\nExperiments Procedure Overview\nExperimental Results and Discussion\nConclusions\n基础知识\n\n全卷积神经网络FCNx\nFCN8\n跳跃卷积层\ndownsampling and upsampling\n交叉熵\n\n其他参考内容\n\n\n自动驾驶毫米波雷达物体检测技术-算法\n\n\n自动驾驶毫米波雷达物体检测技术-硬件\n\n介绍了有关毫米波雷达测距测速的相关基础知识原理，和本论文所讲的这方面内容比较重合,作者william是一名自动驾驶全栈工程师\n\n\n\n","categories":["Paper Reading"],"tags":["sensor fusion","object detection","LiDAR","lidar fusion","EKF","Babak Shahian Jahromi"]},{"title":"The Illustrated Transformer | Jay Alammar","url":"/2021/05/14/illustrated_transformer/","content":"介绍\n\n\n翻译自illustrated transformer， 原作者为Jay Alammar,\n\n\n原文地址：http://jalammar.github.io/illustrated-transformer/\n\n\n主要介绍了Transformer的结构，自注意力机制计算过程，帮助初学者第一次了解Transformer。\n\n\nThe Illustrated Transformer\n​\t在先前的推送中，我们考察了注意力——这是一种现代深度学习模型中常用的方法。注意力是能帮助提升神经网络翻译应用的效果的概念。在本篇推送中，我们将会考察The Transformer——一个使用注意力机制的模型，这些模型是可以被训练的。Tranformer在特定任务中优于谷歌神经机器翻译模型。然而，这个优点主要来自它的并行化。Google Cloud推荐使用The Transformer作为参考模型来使用他们的Cloud TPU  (https://cloud.google.com/tpu/)产品。让我们试着把这个模型拆开，看看它是如何工作的。\n\n\n起源：The Transformer最早在文章 Attention is All You Need (https://arxiv.org/abs/1706.03762)中被提出。\n\n\nTensorflow实现：它的TensorFlow实现可以作为Tensor2Tensor  (https://github.com/tensorflow/tensor2tensor)包的一部分。\n\n\nPyTorch实现：哈佛大学的NLP小组创建了一个指南，用PyTorch实现对论文进行了注释(http://nlp.seas.harvard.edu/2018/04/03/attention.html)。\n\n\n​\t在这篇文章中，我们将尝试把事情简单化一点，逐个介绍这些概念，希望能让那些对主题没有深入了解的人更容易理解\nA High-Level Look\n我们首先将模型看作一个黑盒。在机器翻译应用程序中，它会用一种语言获取一个句子，然后用另一种语言输出翻译结果。\n\n让我们一下子打开这个黑盒子（原作者用了一种风趣的表达，把这个黑盒子比喻为擎天柱Optimus Prime）。我们将会看到一个编码器和一个解码器，他们之间还有一个连接。\n\n编码器是由六个编码器堆组成（在原论文中是六个且每个都在另外一个上面）—六个这个个数其实没有什么神奇的，你可以根据实验需要设置为其他数字。解码器是一个由具有相同个数的堆组成。\n\n每个编码器内部的结构都完全一样（目前他们还不会共享权重），每一个都被分为两个子层：\n\n编码器的输入首先经过自注意层——这个层的作用是：在编码一个特定的单词时，帮助编码器查看输入句子中的其他单词。我们将在后面进一步了解自注意层。\n自注意层的输出送入到前馈神经网络。每个位置的前馈神经网络都是完全相同的。\n解码器也具有这些层，但是在他们之间有一个注意力层帮助解码器关注于输入句子中的相关部分（和在seq2seq models中的attention的作用类似）\nBringing The Tensors Into The Picture\n现在我们已经查看了模型的主要部分，接下来让我们看看向量/张量是如何经过这些部分，最终将输入结果变为输出的。\n因为NLP应用是常见的，我们先采用embedding algorithm嵌入算法将每一个单词都转化为一个向量\n\n现在每个单词都变为了512维的向量，我们用相同的长度表示他们。embedding只在最下层的编码器中发生。但是这个抽象的过程对于每个编码器都一样，他们都接受512维的向量——也就是说，在最底层的编码器，它接收的是512维的单词embedding，在其他编码器中，它直接接收下层的编码器的输出。这个列表的大小是我们可以设置的超参数——基本上就是我们训练数据集中最长的句子的长度。\n在我们对单词进行过嵌入之后，每个单词都会流经编码器的两层。\n\n现在我们开始考察Transformer的关键性质：每个单词在它的位置都只从它自己的路径通过。在自注意层具有这些路径间的相关性（依赖度），前馈层没有那些依赖度，所以，这些不同的路径在流经前馈层的时候可以并行处理。\n接下来，我们将会将例子变为一个短句子，我们看看在每个子层发生了什么。\nNow We’re Encoding!\n如同我们上面所介绍的，一个编码器接收一系列向量作为输入。它通过将这些向量传递到自注意层来处理，然后进入前馈神经网络，然后将输出发送到下一个编码器。\n\nSelf-Attention at a High Level\n不要被我抛出的“自注意”这个词所愚弄了，好像这是一个每个人都应该熟悉的概念一样。实际上我个人在阅读Attention is All You Need一文之前从未接触过这个概念。让我们来总结一下它是如何工作的。\n假设下面的句子是我们想要翻译的输入句子:\n” The animal didn’t cross the street because it was too tired ”\n这个句子中的“it”指的是什么?是指街道还是指动物?这对人类来说是个简单的问题，但对算法来说就没那么简单了。\n当模型在处理“it”这个词时，自注意机制让它把“it”和“animal”联系起来。\n当模型处理每个单词(输入序列中的每个位置)时，自注意机制允许它查看输入序列中的其他位置，以寻找有助于更好地编码这个单词的线索。\n如果你熟悉RNN，想想看如何保持一个隐藏状态使得RNN将它处理过的前一个单词/向量的表示与它正在处理的当前单词/向量结合起来的。Self-attention是Transformer用来将其他相关词汇的“理解”转化为我们当前正在处理的词汇的方法。\n\n当我们在编码器#5中编码单词“it”(堆栈中的顶部编码器)时，注意力机制的一部分关注于“the Animal”，并将其表现形式的一部分整合到“it”的编码中。\n一定要去查查 Tensor2Tensor notebook 看看你如何能加载一个Transformer模型，并且利用这个交互式可视化来检验一下它。\nSelf-Attention in Detail\n让我们先看看如何使用向量来计算自我注意力，然后继续看看它是如何实际实现的——使用矩阵。\n\n\n第一步：对于每一个输入的单词嵌入向量，都生成三个向量，分别是队列向量，键向量和值向量。这三个向量是根据对应的矩阵生成的（这些矩阵在训练过程中经过训练得到）\n\n注意：这些生成的向量的维度比输入的嵌入向量要小，维度是64维，然而编码器和解码器的输入向量的维度是512维。它们不必更小，这只是一种结构选择，可以使多线程注意力(大部分)的计算保持不变。\n​\t那么究竟什么是“队列”，“键”，和“值”呢？：它们是对计算和思考注意力有用的抽象概念。一旦你继续阅读下面的注意力是如何计算的，你就会知道所有你需要知道的关于这些向量所扮演的角色。\n\n第二步：计算分数。我们在计算这个例子中的第一个词“Think”的Self-Attention。我们需要将输入句子中的每个单词与这个单词进行评分。当我们在某个位置编码一个单词时，分数决定了对输入句子的其他部分的关注程度。\n\n\n分数是通过取查询向量与我们评分的各个单词的键向量的点积来计算的。如果我们在处理1号位置的单词的Self-Attention，第一个分数是q1和k1的点积。第二个分数是q1和k2的点积。\n\n第三、第四步：将得分除以8(论文中使用的关键向量维数64的平方根)。这就产生了更稳定的梯度。这里可能有其他可能的值，但这是默认值)，然后通过一个softmax操作传递结果。Softmax将分数标准化，所以它们都是正的，加起来等于1。\n\n\n这个softmax分数决定了在这个位置上每个单词将表达多少。显然，在这个位置的单词将有最高的softmax分数，但有时关注与当前单词相关的另一个单词是有用的。\n\n第五步：将softmax分数和对应的值向量相乘（为了之后对他们进行求和）。直觉上来讲，这里是为了保持我们关注的单词的值向量不变，而忽略掉不相干的单词（通过给他们乘上很小的数字比如0.001）\n第六步：将这些加权之后的值向量进行求和。这便产生了这个位置的自注意层的输出（对于第一个单词的）\n\n这就是自注意计算的结论。这里得到的向量是我们可以发送给前馈神经网络的向量。然而，在实际实现中，这种计算以矩阵形式进行，以获得更快的处理速度。现在我们已经直观地理解了单词层面上的计算，接下来来看一下矩阵层面的计算。\nMatrix Calculation of Self-Attention\n\n第一步：计算队列矩阵，键矩阵和值矩阵，我们的方法是将嵌入向量打包到一个矩阵X中，然后将其乘以我们训练的权重矩阵(WQ, WK, WV)。\n\n\n\n最后一步：因为我们处理的是矩阵，所以我们可以把第2步到第6步压缩到一个公式中，来计算自注意层的输出。\n\n\nThe Beast With Many Heads\n\n（The Beast With Many Heads是指具有很多个头的野兽，这里也是作者的一个有趣的比喻)\n\n这篇文章进一步细化了自注意层，增加了一个“多头”（multi-head）注意机制。这从两个方面提高了注意层的性能:\n\n它提升了模型关注不同位置的能力。是的，在上面的例子中，z1只包含了其他所有编码的一小部分，但是它可能只是被自己自身完全决定。如果我们要翻译像“The animal didn’t cross the street because it was too tired”这样的句子，它会很有用，因为我们想知道“it”指的是哪个词。\n它给了注意层多个“表征子空间”。如我们接下来想看到的一样，在多头注意力机制下，我们不仅具有一个，而是具有多个队列/键/值的权重矩阵（the Transformer使用了8个注意力头，所以我们最后对于每个编码器/解码器都具有8组）。每一组都将随机初始化。然后，在训练之后，每一组都被用来将初始输入的嵌入向量（或者是来自更低一层的编码器/解码器）投影到一个不同的表征子空间\n\n\n\n在多头注意力机制下，我们对于每一个head保持完全分离的Q/K/V权重矩阵，最后得到完全不同的Q/K/V矩阵。如我们之前所述，我们通过对X乘以WQ/WK/WV来产生Q/K/V矩阵\n\n如果我们做如上所述的同样的自注意计算，只是用不同的权重矩阵进行8次不同的计算，我们最终得到8个不同的Z矩阵\n\n这给我们造成了一点挑战。前馈层不需要8个矩阵——它只需要1个矩阵（也就是每个词一个向量）。所以我们需要一个将8个矩阵变为一个的方式。\n我们怎么办呢？我们将这8个矩阵叠放，然后用一个额外的权重矩阵WO来乘以他们\n\n以上全部就是多头注意力机制的全部内容了。我承认，这里面由很多很多的矩阵，我们把他们放在一张图上面在这些我们就可以一次看清了。\n\n现在我们已经接触了多头注意力，我们重新回顾一下我们之前的例子来看看不同的注意力头关注着什么（当我们编码句子中it的时候）\n\n\n当我们编码&quot;it&quot;的时候，一个注意力头最关注于“the animal”，然而另外一个关注于“tired”——某种意义上来讲，模型对单词“it”的表征同时体了“animal”和“tired”。\n\n如果我们继续增加注意力头，额，这就很难解释了。\n\nRepresenting The Order of The Sequence Using Positional Encoding\n到目前为止，我们所描述的模型还缺少一种解释输入序列中单词顺序的方法。\n为了编码这个，the transformer为每一个输入的嵌入向量增加了一个向量。这些向量遵循着一种模型学习到的特定的模式，这有助于它确定每一个单词的位置，以及他们在句子之间的远近。这里的直觉是通过给嵌入向量增加这些向量，在计算Q/K/V向量和点乘注意力机制过程中，提供了有意义的嵌入向量的距离。\n\n如果我们假设嵌入向量有4维，那么实际的位置向量会是这个样子\n\n那么它的模式看上去是什么样的呢？\n在下面的图中，每一行对应向量的位置编码。所以第一行是我们要添加到输入序列中嵌入第一个单词的向量。每行包含512个值—每个值的范围是1到-1。我们用不同的颜色进行了标记，这样图案就清晰可见了。\n\n\n这是一个实际的位置编码示例，用于20个单词(行)，嵌入大小为512(列)。你可以看到它从中间一分为二。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分的值由另一个函数(使用余弦)生成。然后将它们连接起来形成每个位置编码向量。\n\n本文(第3.5节)描述了位置编码的公式。您可以在get_timing_signal_1d()中看到生成位置编码的代码。这并不是位置编码的唯一可能方法。然而，它的优势在于能够扩展到看不见的序列长度(例如，如果我们的训练模型被要求翻译一个比训练集中任何一个句子都长的句子)。\n2020.7更新：上面显示的位置编码来自Transformer的transformer2transformer实现。本文给出的方法略有不同，它不是直接连接两个信号，而是将两个信号交织在一起。下图显示了它的外观。\n\nThe Residuals\n在我们继续之前，编码器结构的一个细节我们需要注意的是，对于每个编码器中的每一个子层(self-attention,ffnn)都具有一个残差连接，这一步是通过 layer-normalization步骤实现的\n\n如果我们可视化向量在正则化层的操作，它会是这样的：\n\n这也适用于解码器的子层。如果我们考虑一个包含2个堆叠的编码器和解码器的Transformer，它看起来应该是这样的:\n\nThe Decoder Side\n既然我们已经介绍了编码器方面的大部分概念，我们基本上也知道了解码器的组件是如何工作的。但让我们看看它们是如何一起工作的。\n编码器首先处理输入序列。然后，顶部编码器的输出被转换为一组注意向量K和V，这些将被每个解码器在其“编码器-解码器注意”层中使用，帮助解码器关注输入序列中的适当位置:\n\n下面的步骤重复这个过程，直到到达一个特殊的符号，表示Transformer解码器已经完成了它的输出。每个步骤的输出在下一个时间步骤中被提供给底部解码器，解码器冒泡他们的解码结果，就像编码器所做的。就像我们对编码器输入所做的那样，我们在这些解码器输入中嵌入并添加位置编码，以表示每个单词的位置。\n\n解码器中的自注意层与编码器中的自注意层的操作方式略有不同:\n在解码器中，自注意层只允许关注输出序列中较早的位置。这是通过在self-attention计算的softmax步骤之前屏蔽未来的位置(将它们设置为-inf)来实现的。\n“编码器-解码器注意”层的工作方式就像多头自注意一样，除了它从下面的层创建队列矩阵，并从编码器堆栈的输出中获取键和值矩阵。\nThe Final Linear and Softmax Layer\n解码器堆栈输出一个浮点数向量。我们怎么把它变成一个单词呢?这是最后一个线性层的工作，后面是一个Softmax层。\n线性层是一个简单的完全连接的神经网络，它将解码器堆栈产生的向量投射到一个更大的向量，称为logits向量。\n让我们假设我们的模型知道10,000个唯一的英语单词(我们的模型的“输出词汇表”)，这些单词是从它的训练数据集中学习的。这将使logits向量有10,000个单元格宽——每个单元格对应一个唯一单词的分数。这就是我们如何解释线性层之后的模型输出。\n然后softmax层将这些分数转化为概率(所有分数都是正的，加起来都是1.0)。选择概率最高的单元格，并生成与之相关的单词作为这个时间步长的输出。\n\nRecap Of Training\n既然我们已经通过一个经过训练的Transformer介绍了整个前向传递过程，那么接下来我们看看如何对这个模型进行训练\n在训练期间，未经训练的模型将经历完全相同的向前传递。但是因为我们是在一个标记的训练数据集上训练它，所以我们可以将它的输出与实际正确的输出进行比较。\n为了将这个过程可视化，我们假设输出词汇只包含六个词(“a”, “am”, “i”, “thanks”, “student”, and “” (short for ‘end of sentence’)))\n\n一旦定义了输出词汇表，就可以使用相同宽度的向量来表示词汇表中的每个单词。这也被称为一次性编码（one-hot encoding）。例如，我们可以用下面的向量来表示单词“am”:\n\n接下来，让我们讨论模型的损失函数——我们在训练阶段优化的度量，以形成一个经过训练的、希望非常精确的模型。\nThe Loss Function\n假设我们正在训练模型。说这是我们训练阶段的第一步，我们用一个简单的例子来训练——把“merci”翻译成“thanks”。\n这意味着，我们希望输出是一个表示“谢谢”这个词的概率分布。但由于这种模式尚未经过训练，目前还不太可能实现。\n\n\n由于模型的参数(权重)都是随机初始化的，(未经训练的)模型为每个单元格/字生成一个具有任意值的概率分布。我们可以将其与实际输出进行比较，然后使用反向传播调整所有模型的权重，使输出更接近期望的输出。\n\n如何比较两种概率分布?我们只需要把一个和另一个相减。要了解更多细节，请查看交叉熵 cross-entropy和Kullback-Leibler散度Kullback–Leibler divergence。\n但请注意，这是一个过于简化的示例。更实际的是，我们会使用比一个词更长的句子。例如-输入:&quot; je suis étudiant “，期望输出:” i am a student &quot;。这真正的意思是，我们想要我们的模型连续输出概率分布如下:\n\n每个概率分布都由一个宽度为vocab_size的向量表示(在我们的简单示例中是6，但更实际的数字是30,000或50,000)\n第一个概率分布在与单词“i”相关的细胞中具有最高的概率\n第二个概率分布在细胞中与&quot; am &quot;相关的概率最高\n以此类推，直到第5个输出分布表示“&lt;句子结束&gt;”符号，该符号还具有10,000个元素词汇表中与之相关联的单元格。\n\n\n\n目标概率分布，我们将针对例句训练我们的模型。\n\n在一个足够大的数据集上训练模型足够长的时间后，我们希望产生的概率分布会像这样\n\n\n希望在训练之后，模型能够输出我们所期望的正确翻译。当然，这并不能真正说明这个短语是否是训练数据集的一部分(见:交叉验证)。注意，每个位置都还有一点概率，即使它不太可能是那个时间步长的输出——这是softmax的一个非常有用的特性，它有助于训练过程。\n\n现在，因为模型每次产生一个输出，我们可以假设模型从概率分布中选择概率最高的单词，然后扔掉其他的。这是一种方法(称为贪婪解码greedy decoding)。另一种方法是保留最上面的两个单词(例如，‘I’和‘a’)，然后在下一步中，运行模型两次:一次假设第一个输出位置是单词“I”，另一次假设第一个输出位置是单词“a”，考虑到位置#1和#2，哪个版本产生的误差较小就保留哪个版本。我们对第2和第3个位置重复这一点。这种方法称为“beam search”，在我们的示例中，beam_size是2(意味着在内存中始终保存两个部分假设(未完成的翻译))，top_beams也是2(意味着我们将返回两个翻译)。这些都是你可以实验的超参数。\nGo Forth And Transform\n我希望你已经发现这里是一个有用的地方，并且开始了解Transformer的主要概念了。如果你想要更深入，我建议以下步骤:\n\nRead the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\nWatch Łukasz Kaiser’s talk walking through the model and its details\nPlay with the Jupyter Notebook provided as part of the Tensor2Tensor repo\nExplore the Tensor2Tensor repo.\n\nFollow-up works:\n\nDepthwise Separable Convolutions for Neural Machine Translation\nOne Model To Learn Them All\nDiscrete Autoencoders for Sequence Models\nGenerating Wikipedia by Summarizing Long Sequences\nImage Transformer\nTraining Tips for the Transformer Model\nSelf-Attention with Relative Position Representations\nFast Decoding in Sequence Models using Discrete Latent Variables\nAdafactor: Adaptive Learning Rates with Sublinear Memory Cost\n\n","categories":["Paper Reading"],"tags":["Transformer - NLP"]},{"title":"两种前沿的异类传感器信息融合方法","url":"/2020/11/22/%E4%B8%A4%E7%A7%8D%E5%89%8D%E6%B2%BF%E7%9A%84%E5%BC%82%E7%B1%BB%E4%BC%A0%E6%84%9F%E5%99%A8%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95/","content":"\n论文笔记，原创内容；未经许可，请勿转载\n\n说明\n近期接触异类传感器信息融合领域，这里将之前论文进行提炼和总结，主要是对以下两篇论文的方法进行讲解。\n由于本人最近接触这个领域，所述内容可能比较基础，请见谅。\n主要讲述文献：\n\nLi Y, Zhao H, Hu Z, et al. IVFuseNet: Fusion of infrared and visible light images for depth prediction[J]. Information Fusion, 2020, 58: 1-12.\nNobis F, Geisslinger M, Weber M, et al. A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection[C]//2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF). IEEE, 2019: 1-7.\n\n红外与可见光进行融合：IVFuseNet\n红外与可见光信息的互补性\n红外与可见光信息存在**“互补性”**，具体体现为\n\n\n可见光图像特点\n\n大多数研究工作基于可见光进行识别和分类，相关方法比较成熟\n光照情况好的情况下，可见光对物体的轮廓，纹理信息的提取更好\n\n\n\n红外图像特点\n\n红外图像不受亮度变化影响\n红外图像具有高穿透力\n红外检测距离远高于可见光的检测距离\n红外图像缺乏纹理信息，对比度较低\n\n\n\n\n可见光与红外的对比\nIVFuseNet\nLi Y(华东理工大学-赵海涛课题组)提出了一种基于CNN的融合架构，称为IVFuseNet(下图)，能自适应融合红外和可见光图像的互补信息，进行深度预测，它由共同特征融合子网络、全特征融合子网络和高分辨率重建子网络3个子网络组成。\n\n\n共同特征融合子网络：\n\n\n\n部分耦合滤波器 : 为了保持显著特征，我们在共特征融合子网络中提出部分耦合滤波器，在自适应加权融合前进一步增强特征。耦合滤波器有助于发现更有鉴别性的特征，这些特征很容易被彼此忽略。\n\n\n\n全特征融合子网络\n\n\n\n自适应加权融合策略： 首先，我们考虑了红外图像和可见光图像在不同场景下对深度预测的贡献不同。提出了自适应加权融合策略，其中系数矩阵表示两种图像的不同贡献。\n\n\n\n高分辨率重建子网络：\n\n\n\n残差密集卷积 RDC : 此外，融合特征的分辨率在卷积过程中降低，小于ground truth深度图像的分辨率。在深度预测之前，我们需要恢复融合特征的分辨率。受[28,29]的启发，**残差密集卷积(residual dense convolution, RDC)**在高分辨率重建中具有良好的性能。因此在高分辨率重建子网络中，我们在高分辨率重建子网络中采用RDC，也可以增强融合的特征。\n\n\n共同特征融合子网络\n网络结构\n\n\n双流程结构\nAlexNet为每个流程的基础：对于每个流，我们使用AlexNet[33]作为基础，因为AlexNet的参数数量相对较小。共特征融合子网络的具体内容如表1所示。此外，我们还可以选择其他网络作为基准，如VggNet[34]。该子网以256×512分辨率的红外图像及其对应的可见光图像作为子网的输入。\n部分耦合过滤器：与传统的双流CNN不同，我们在每个卷积层中设计部分耦合滤波器来学习红外和可见光图像之间的可转移特征。耦合比率如表2所示\n反向传播BP：独立特征需要迭代修正一次，共享特征需要修正两次\n\n\n共同特征提取\n\n耦合比设置\n\n定义：R(耦合比) = k（耦合过滤器） / n （所有过滤器）\n耦合比应该随着网络深度的增加而增加\n设定为0, 0.25, 0.5, 0.75\n解释：\n\n浅层卷积层：提取图像的纹理和细节特征，红外与可见光中有很大的不同。\n深层卷积层：提取图像的结构和形状特征，红外与可见光有很大的相同。\n\n\n\n\n全特征融合子网络\n融合步骤\n设f_ir∈b×w×h×c和f_vi∈b×w×h×c是从共同特征融合子网络提取的特征\n\n首先:我们将fir和fvi在第三个维度上进行拼接，这相当于融合f_fusion∈b×w×h×2c的红外图像和可见光图像的特征。\n第二:将融合特征与核k∈2c×c×1×1进行卷积，其中2c为输入通道数，c为输出通道数，核大小为1×1。受[43]的启发，由于融合特征f_fusion同时考虑了红外和可见光图像的特征，因此该卷积运算的输出与这两种特征都有关。因此，这个程序学习了两种特征之间的相关性。在得到f_ir或f_vi的相同维数的初始系数矩阵M后，我们计算它们的点积，表示它们对不同场景深度预测的不同贡献。\n第三，我们设计了一个sigmoid层，将M中的每个元素转换成0到1之间的概率形式。经过这三个步骤，最终得到系数矩阵G。其过程如下:\n\n\n在不同的条件下，系数矩阵是不同的.例如,G_ir可能在暗光情况下比G_vi大而,G_vi可能在白天下比G_ir大.G_ir和G_vi分别表示红外图像和可见光图像特征的贡献，具体如下:\n\n其中’圈点’代表点积.全融合特征可以这样得到:\n\n全特征融合子网络决定了红外特征和可见光特征对深度预测的依赖程度。\n高分辨率重建子网络\n网络结构\n\n蓝色：卷积层；绿色：BN层；黄色：激活函数\n残差密集卷积网络(Residual Dense Network,RDN)\n\n大多数基于CNN的深度神经网络模型并没有充分利用原始低分辨率（LR）图像的分层特征，因此性能相对较低。\n\n\n残差密集网络优点：\n（1）充分的利用了所有卷积层的层次信息，通过密集连接卷积层提取丰富的卷积特征。\n（2）允许从前一个RDB的状态直接连接到当前RDB的所有层，从而形成连续的内存（CM）机制。\n（3）可以更有效地从先前和当前局部特征中学习更有效的特征，并稳定更广泛的网络的训练\n将浅层特征和深层特征结合在一起，从原始 LR 图像中得到全局密集特征。\n网络效果评价\n评价标准\n在NUST-SR数据集的测试图像上，我们使用四个指标对提出的IVFuseNet进行评估:\n\n方均根误差(RMSE)\n平均相对误差(Rel)\n平均对数误差\n识别准确率\n\n红外与可见光融合的结果\n\n\n\n密集卷积块的结果\n\n\n和相关方法的对比\n\n\n主要贡献\n\n我们提出了一种基于CNN的结构，称为IVFuseNet，它可以自适应融合红外和可见光图像的互补性，以解决不同光照条件下的深度预测问题。我们提出的融合方法主要包括两个方面。\n\n首先，我们在共特征融合子网络中设计部分耦合过滤器，利用“辅助变量”增强红外和可见光图像的特征。\n其次，为了考虑不同光照条件下红外和可见光图像的不同贡献，设计了自适应加权融合方法。\n此外，我们引入3个残差密集卷积块，进一步恢复细节，提高深度预测的准确性.\n\n\nIVFuseNet的有效性已经在我们的NUST-SR数据集上得到了验证。与其他方法相比，我们在该数据集上取得了最好的性能。\n\n雷达与可见光融合：CRF-Net\n雷达与可见光数据的特点\n雷达：直接获取目标的距离和径向速度信息。它能够在与地面平行的二维平面上定位物体。与相机相比，雷达传感器无法获得高度信息。\n相机数据特点：在视觉平面中获得二维的密集的纹理，色彩，轮廓信息图。\n雷达数据处理\n雷达数据的处理主要有四点问题：\n\n雷达和相机数据的空间校准\n如何应对雷达回波丢失的高度信息\n如何应对雷达数据的稀疏性\n用真实标注信息滤波(ground-truth filtering, GF)方法来消除雷达数据的噪声和杂波\n\n雷达和相机数据空间校准\n雷达输出的是具有雷达性质的2D-稀疏-点云信息，本工作中应用的雷达信息主要有方位角，距离，雷达散射截面(radar cross section, RCS)。我们将雷达信息从2D地面平面转换到了图像面的垂直面上。雷达回波信息被在扩维图像(augmented image)上面被保存为像素。无雷达回波的位置上，投影的雷达通道值的像素被标记为0。输入的相机图片信息包含三通道(红，绿，蓝)；我们将先前提到的雷达通道作为神经网络的输入。在我们的数据集中，三个雷达的the field of view(FOV)和前视鱼眼相机的FOV重合。我们将三个雷达点云信息合并到一个中去，然后利用这个作为投影雷达输入信息源。投影的算法不尽相同，例如nuScenes利用70°FOV相机，但是TUM利用180°FOV鱼眼相机。在nuScenes中，提供了摄像机内外映射矩阵，用于将点从世界坐标转换为图像坐标。而鱼眼透镜的非线性不能用线性矩阵运算来映射。我们使用[28]提供的校准方法将世界坐标映射到我们自己的图像坐标。\n总结重点：\n\n三个雷达点云信息合并为一个，作为雷达信息源\n雷达信息源投影到图像平面上\n鱼眼相机的世界坐标投影到图像坐标上\n投影雷达数据作为图像信息的扩维，无回波的地方像素就为0\n\n应对雷达高度信息缺失\n\n\n首先考虑到我们检测的目标主要是车，卡车，摩托车，自行车和行人\n\n\n我们假设了雷达得到的回波点的高度都是3m，以覆盖这些目标的高度\n\n\n然后这个高度线也作为像素投影到了图像平面\n\n\n应对雷达信息的稀疏性\n\nnuScene中激光雷达一次返回约14000点(相同水平张角下)[29], 这大概等同于雷达每周期探测57次的信息量.\n为了解决稀疏性问题,[25]利用了概率网格图(probabilistic grid maps)来获得雷达的连续性信息.\n我们通过融合最后13次雷达周期(约1s)的图像来提高雷达数据的密度.\n在这个方法中,我们补偿了自身运动(Ego-motion),但是目标车辆的运动无法补偿\n这种方法也增加了噪声,因为前一时刻对移动目标的检测与当前目标物体的位置不一致.但是为了增加额外信息,这个缺点是可以忍受的.\n下图展示了神经网络的输入数据的形式,雷达数据(距离和RCS)被标记在相同位置所以这里有均匀的颜色\n\n\n雷达数据的滤波\n雷达回波会返回需要与目标无关的检测信息,例如幽灵目标,无关目标和地面检测. 这些检测信息就是所说的噪声或杂波。在评价中，我们比较了融合原始雷达数据和利用两种额外滤波方法的雷达数据融合的效果。首先在nuScene中，只有一些标记目标被雷达检测到。在训练和估计中，我们因此实施了一种注解标注滤波器（annotation  filter，AF），这样经过滤波后的真实标注信息只包含至少同时被雷达点检测到的目标。这种方法对于可能被两种模式检测到的目标能发挥它的潜力。第二，我们采用了真实标注信息滤波器ground-truth filter来移除3D真实标注信息边界外的雷达探测点。当然，这个步骤如果实际场景是无法进行的。本文的目的是在输入信号杂波较少的情况下，证明融合概念的普遍可行性。经过雷达滤波后的图像在图像2b中被展示。注意，**GRF（ground-truth radar filter）**并没有输出完美的雷达数据，其中滤掉了部分数据的相关检测，原因有四：…（总结中有）\n\n总结：\n\n两种滤波器：\n\nAF:过滤没有同时被雷达数据检测到的目标\nGRF:过滤超出真实标注边界的目标\n\n\n滤波无法输出完美雷达数据的四点原因：\n\n没有对运动目标的补偿。雷达探测频率2Hz，探测周期中间的目标可能就消失了\n雷达数据和相机数据具有轻微的空间错校准\n雷达数据和相机数据不是刚好都是同一时间产生的\n虽然雷达距离测量非常可靠，但其测量并不完美，轻微的误差就会导致探测落在真实标注边界之外。在图2b中可以看到对部分相关数据进行了无意的过滤\n\n\n\n网络结构\n\n为什么不能直接融合（在第一层就开始融合）？\n\n\n雷达投影数据中的像素与图像数据中的像素意义是不同的，前者代表了目标的距离，对于驾驶任务更为相关。如果要直接融合这两种数据，我们需要假设他们是语义近似的；然而，由于上一点的原因，我们显然难以做出这种假设！\n\n\n那如何融合？\n\n\n在神经网络的更深层，输入数据被压缩为更密的形式，这种形式理想上包含所有相关的输入信息。即然我们无法确定两种传感器类型的信息的抽象层次，我们将网络设计为自主学习在哪种层次进行融合最有利于减少全局损失的形式。\n\n\n\n网络介绍\n\n相机和雷达数据在顶排被输送给该网络。\n在左侧，原始雷达数据通过[^max-pooling]来改变尺寸以送进更深的网络层。\n雷达数据被合并到前一级融合网络层的主要分支上\n特征金字塔网络(Feature Pyramid Network, FPN)[33],其中雷达数据被分级融合进去，通过将雷达通道合并到额外的通道上去\n最后FPN的输出通过回归和分类块进行处理[30]\n\n\n网络优势\n\n\n通过调节雷达特征在各个层的权值，优化器隐式地教会了该网络在何种深度将雷达数据进行融合具有最好的影响\n\n\n类似的技术被[16]所采用\n\n\n\n\n训练策略\n\nBlackIn :有意在部分训练中禁用了所有图像数据输入神经元，这个比例大概是0.2。这样做的目的是：图像数据的丢失能更加刺激该网络更依赖于雷达数据。‘教育’该网络稀疏的雷达数据是独立于密集的图像数据的。\n用预先在图像上训练的权重开始训练，用于特征提取器。\n\n效果评价\n数据集\n\n评价\n\n一个例子\n下图的对比能很好低表现CRF-Net的优势：\n\n主要贡献\n\n提出了CRF-Net结构来融合雷达和相机数据\n本研究适应了激光雷达和相机数据处理的思路，为雷达数据融合研究开辟了新的方向。\n对雷达数据处理的难点和解决方案进行了讨论\n引入了BlackIn训练策略进行雷达和相机数据的融合。\n证明了采用神经网络对雷达和摄像机数据进行融合，可以有效地提高目前最先进的目标检测网络的精度\n由于对雷达与相机数据的神经网络融合的研究是最近才开始的，寻找优化的网络架构还需要进一步的探索。\n\n基础知识\n特征融合方式(concat 与 add)\nconcat层的作用就是将两个及以上的特征图按照在channel或num维度上进行拼接\nconcat : channel或者宽度增加，不是叠加\nadd：不改变维度，只是叠加\n当两路输入可以具有“对应通道的特征图语义类似”（可能不太严谨）的性质的时候，可以用add来替代concat，这样更节省参数和计算量（concat是add的2倍）。\n参考资料：https://blog.csdn.net/qq_32256033/article/details/89516738\nhttps://www.zhihu.com/question/306213462/answer/562776112\n反卷积(deconvolution)\n反卷积(deconvolution)：更好的说法是转置卷积，是一种上采样的方式，同样利用卷积使得图片的分辨率变大\n4*4的输入矩阵可展开为16维向量x\n2*2的输出矩阵可展开为4维向量y\n卷积运算可表示为y = Cx,则反卷积运算可以表示为x = C‘y，将图片的维数放大\n上采样(upsampling)\n上采样：上采样是指将图像上采样到更高分辨率的任何技术。最简单的方法是使用重新采样和插值。即取原始图像输入，将其重新缩放到所需的大小，然后使用插值方法（如双线性插值）计算每个点处的像素值。\n将图片的分辨率变大的操作，有关上采样的改进案例：https://distill.pub/2016/deconv-checkerboard/\n批归一化(batch normalization,BN)\n批归一化(batch normalization,BN):一般在激活层之前，主要是让数据的分布变得一致，从而使得训练深层网络模型更加容易和稳定。参考：\n\nhttps://www.cnblogs.com/mfryf/p/11381361.html#ct2\nhttps://blog.csdn.net/qq_42219077/article/details/88379566\n\n池化（pooling）\n池化的原理或者是过程：pooling是在不同的通道上分开执行的（就是池化操作不改变通道数），且不需要参数控制。然后根据窗口大小进行相应的操作。一般有max pooling、average pooling等。\n\n池化层主要的作用\n\n\n首要作用，下采样（downsamping）\n降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减少计算量、减少内存消耗等等。各种说辞吧，总的理解就是减少数量。\n实现非线性（这个可以想一下，relu函数，是不是有点类似的感觉？）。\n可以扩大感知野\n可以实现不变性，其中不变性包括，平移不变性、旋转不变性和尺度不变性。\n\n参考连接：\n\nhttps://zhuanlan.zhihu.com/p/27642620\nhttps://www.zhihu.com/question/36686900\nhttps://blog.csdn.net/LIYUAN123ZHOUHUI/article/details/61920796\n\nVGG\nVGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。下图是一个用3层3x3代替5x5的例子\nRetinaNet(Focal Loss)：\nRetinaNet由以下处理步骤组成提取、特征金字塔网络(FPN)和用于分类和回归的检测头。\n\nFocal loss\n\nFocal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘。\n目标识别有两大经典结构:\n\n\n第一类是以Faster RCNN为代表的两级识别方法，这种结构的第一级专注于proposal的提取，第二级则对提取出的proposal进行分类和精确坐标回归。两级结构准确度较高，但因为第二级需要单独对每个proposal进行分类/回归，速度就打了折扣；\n\n\n第二类结构是以YOLO和SSD为代表的单级结构，它们摒弃了提取proposal的过程，只用一级就完成了识别/回归，虽然速度较快但准确率远远比不上两级结构。\n\n\n那有没有办法在单级结构中也能实现较高的准确度呢？Focal Loss就是要解决这个问题。\n\n检测框架\nRetinaNet本质上是 Resnet + FPN + 两个FCN子网络。\n以下为RetinaNet目标框架框架图。有了之前blog里面提到的FPN与FCN的知识后，我们很容易理解此框架的设计含义。\n\n\n参考内容：\nRetinaNet: Focal loss在目标检测网络中的应用\nRetinaNet(Focal Loss)\n","categories":["Paper Reading"],"tags":["sensor fusion","review"]}]